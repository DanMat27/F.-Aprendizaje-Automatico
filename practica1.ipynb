{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 1. Pareja 02: Daniel Mateo y Laura Sánchez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos todas las librerias necesarias para este Notebook.\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from Datos import Datos\n",
    "from EstrategiaParticionado import *\n",
    "from Clasificador import *\n",
    "from ClasificadorNB import *\n",
    "import math\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APARTADO 1\n",
    "En este primer apartado se generaron las estrategias de particionado de Validación Simple, que divide los datos según una proporción de Test; y Validación Cruzada, que los divide en K-1 folds para Train y el restante para Test.\n",
    "\n",
    "- Pero antes, hay que leer ambos ficheros (tic-tac-toe.data y german.data) para generar sus correspondientes clases de Datos, y así poder realizar estas particiones sobre sus datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2 ... 1 1 1]\n",
      " [2 2 2 ... 2 1 1]\n",
      " [2 2 2 ... 1 2 1]\n",
      " ...\n",
      " [1 2 1 ... 1 2 0]\n",
      " [1 2 1 ... 1 2 0]\n",
      " [1 1 2 ... 2 2 0]]\n",
      "[[ 0  6  4 ...  1  0  1]\n",
      " [ 1 48  2 ...  0  0  2]\n",
      " [ 3 12  4 ...  0  0  1]\n",
      " ...\n",
      " [ 3 12  2 ...  0  0  1]\n",
      " [ 0 45  2 ...  1  0  2]\n",
      " [ 1 45  4 ...  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "# Dataset de Tic-tac-toe.data.\n",
    "datasettictac = Datos(\"./tic-tac-toe.data\")\n",
    "'''print(datasettictac.nominalAtributos)\n",
    "print(datasettictac.diccionario)'''\n",
    "print(datasettictac.datos)\n",
    "\n",
    "# Dataset de German.data.\n",
    "datasetgerman = Datos(\"./german.data\")\n",
    "'''print(datasetgerman.nominalAtributos)\n",
    "print(datasetgerman.diccionario)'''\n",
    "print(datasetgerman.datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con los ficheros leídos se procede a generar los objetos de las dos validaciones para cada dataset con unos atributos elegidos al azar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación Simple con una proporción del 50% y 2 particiones.\n",
    "vs = ValidacionSimple(0.5, 2)\n",
    "# Validación Cruzada con K=3 particiones totales.\n",
    "vc = ValidacionCruzada(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como más adelante se harán las particiones dentro del clasificador al validar, hemos aplicado el creaPaticiones() para que se pueda observar el correcto funcionamiento de estas estrategias de particionado en este apartado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Validacion Simple Tic-Tac-Toe\n",
      "Hay 479 indices de Train:\n",
      "[377, 18, 901, 205, 46, 138, 320, 228, 133, 196, 38, 845, 154, 231, 114, 607, 443, 775, 317, 545, 465, 175, 819, 313, 366, 725, 95, 710, 565, 294, 266, 598, 531, 158, 218, 682, 461, 161, 483, 209, 79, 63, 615, 434, 612, 832, 690, 622, 312, 170, 792, 142, 378, 30, 556, 28, 345, 767, 858, 500, 546, 124, 176, 343, 297, 686, 708, 81, 864, 285, 558, 103, 213, 37, 647, 250, 680, 727, 887, 234, 31, 925, 915, 817, 370, 359, 230, 758, 504, 417, 255, 36, 592, 314, 129, 621, 860, 905, 92, 477, 908, 307, 375, 551, 917, 935, 326, 10, 664, 729, 761, 712, 334, 300, 403, 741, 583, 911, 82, 408, 6, 831, 865, 219, 780, 872, 850, 474, 449, 401, 484, 848, 718, 857, 405, 728, 873, 39, 373, 719, 603, 928, 550, 505, 587, 264, 248, 514, 371, 906, 796, 282, 924, 190, 577, 730, 55, 466, 617, 352, 822, 112, 804, 513, 238, 90, 520, 759, 706, 751, 836, 360, 912, 569, 926, 648, 800, 199, 447, 837, 503, 445, 921, 631, 625, 588, 862, 689, 753, 650, 843, 64, 107, 242, 444, 475, 955, 814, 715, 16, 57, 574, 89, 739, 600, 185, 944, 554, 540, 632, 374, 72, 821, 348, 537, 383, 486, 570, 149, 673, 787, 896, 440, 745, 150, 197, 83, 247, 153, 322, 32, 721, 886, 953, 847, 290, 152, 468, 274, 652, 319, 67, 349, 770, 5, 99, 794, 41, 0, 48, 350, 933, 665, 140, 78, 494, 687, 700, 754, 789, 499, 869, 384, 464, 271, 407, 200, 937, 888, 601, 595, 321, 42, 524, 849, 579, 216, 280, 120, 260, 43, 675, 493, 489, 733, 325, 797, 677, 130, 609, 328, 166, 889, 455, 410, 86, 833, 762, 168, 244, 517, 940, 582, 21, 818, 623, 791, 291, 620, 198, 863, 214, 20, 26, 126, 870, 173, 385, 14, 139, 179, 954, 547, 699, 191, 97, 614, 930, 549, 332, 737, 347, 71, 395, 834, 957, 239, 301, 104, 671, 471, 724, 226, 3, 811, 74, 559, 482, 98, 363, 432, 462, 507, 354, 479, 892, 35, 567, 663, 207, 202, 807, 165, 536, 810, 606, 732, 757, 918, 223, 639, 277, 633, 396, 555, 19, 778, 562, 497, 766, 212, 783, 553, 815, 702, 678, 188, 180, 916, 539, 666, 608, 931, 731, 431, 734, 902, 711, 777, 509, 936, 526, 535, 84, 287, 802, 227, 707, 268, 24, 132, 4, 414, 330, 920, 393, 356, 44, 756, 125, 261, 704, 738, 419, 626, 508, 397, 51, 75, 938, 552, 430, 436, 584, 563, 616, 793, 881, 11, 485, 15, 178, 879, 899, 805, 784, 338, 922, 934, 749, 776, 119, 427, 253, 428, 70, 102, 532, 105, 491, 571, 661, 591, 412, 337, 512, 529, 891, 451, 284, 151, 788, 108, 25, 318, 49, 162, 361, 701]\n",
      "\n",
      "Hay 479 Indices de Test:\n",
      "[241, 714, 496, 893, 194, 696, 367, 643, 249, 381, 281, 498, 23, 752, 141, 844, 296, 538, 488, 746, 543, 76, 842, 308, 851, 808, 232, 47, 304, 9, 424, 423, 644, 589, 243, 929, 823, 856, 736, 882, 336, 470, 883, 717, 709, 867, 840, 564, 416, 40, 576, 235, 257, 907, 894, 698, 578, 568, 220, 674, 871, 246, 187, 923, 85, 77, 206, 402, 472, 542, 586, 772, 433, 596, 201, 364, 501, 611, 684, 478, 713, 657, 723, 693, 876, 835, 355, 469, 53, 66, 895, 273, 251, 184, 283, 286, 816, 145, 73, 909, 544, 390, 487, 365, 217, 311, 635, 619, 339, 171, 215, 155, 362, 446, 829, 159, 121, 369, 59, 157, 572, 62, 658, 827, 602, 533, 204, 826, 429, 52, 289, 323, 346, 703, 790, 411, 605, 764, 310, 659, 490, 208, 692, 94, 874, 34, 229, 884, 830, 438, 305, 610, 467, 824, 898, 655, 1, 910, 645, 688, 670, 259, 270, 573, 386, 952, 518, 660, 340, 329, 740, 927, 137, 654, 335, 195, 56, 949, 183, 298, 679, 641, 763, 942, 877, 861, 580, 358, 613, 236, 716, 420, 528, 437, 594, 521, 903, 88, 585, 662, 101, 557, 292, 735, 8, 115, 812, 809, 306, 182, 368, 838, 435, 415, 267, 143, 269, 459, 117, 561, 388, 263, 720, 118, 442, 473, 786, 186, 65, 193, 638, 399, 722, 868, 245, 134, 854, 694, 640, 941, 697, 406, 93, 425, 147, 422, 33, 174, 293, 642, 148, 695, 806, 511, 272, 61, 480, 426, 450, 309, 341, 515, 441, 278, 672, 439, 58, 597, 123, 627, 853, 389, 649, 530, 913, 773, 803, 240, 951, 463, 541, 839, 156, 181, 779, 69, 456, 656, 382, 932, 476, 798, 331, 866, 453, 945, 683, 131, 637, 128, 629, 900, 392, 398, 256, 618, 45, 80, 801, 897, 146, 17, 653, 252, 604, 127, 110, 233, 820, 50, 646, 523, 525, 29, 400, 755, 705, 295, 327, 7, 590, 527, 747, 111, 60, 351, 630, 353, 681, 636, 481, 418, 224, 765, 27, 939, 342, 316, 785, 875, 315, 279, 413, 750, 275, 100, 890, 560, 492, 404, 303, 203, 91, 506, 324, 859, 813, 421, 122, 302, 458, 457, 409, 265, 452, 12, 387, 769, 258, 522, 548, 691, 878, 855, 904, 946, 685, 454, 880, 22, 795, 825, 394, 771, 144, 333, 534, 852, 566, 211, 357, 254, 2, 113, 668, 448, 502, 676, 376, 391, 510, 380, 136, 344, 799, 914, 519, 160, 760, 628, 726, 744, 192, 743, 768, 109, 919, 54, 774, 841, 624, 748, 167, 948, 667, 956, 846, 372, 172, 781, 169, 177, 135, 782, 947, 299, 950, 163, 651, 885, 164, 116, 225, 288, 669, 262, 742, 221, 68, 460, 828, 210, 943, 87, 581, 13, 106, 276, 495, 379, 222, 593, 599, 237, 189, 96, 634, 575, 516]\n",
      "##################################\n",
      "Hay 479 indices de Train:\n",
      "[228, 124, 505, 777, 497, 622, 217, 875, 353, 96, 957, 95, 738, 46, 5, 502, 755, 575, 700, 283, 910, 814, 448, 423, 828, 922, 893, 271, 483, 574, 358, 827, 711, 10, 420, 386, 26, 259, 790, 629, 117, 747, 503, 437, 909, 916, 309, 844, 929, 250, 444, 351, 487, 510, 243, 585, 13, 284, 804, 366, 474, 704, 955, 851, 98, 219, 572, 602, 18, 461, 682, 883, 130, 753, 436, 227, 956, 1, 676, 265, 427, 113, 146, 182, 938, 257, 917, 274, 231, 367, 119, 475, 135, 49, 431, 520, 920, 588, 493, 97, 286, 298, 801, 685, 380, 191, 131, 524, 768, 508, 37, 418, 866, 25, 605, 179, 868, 330, 653, 235, 392, 304, 628, 674, 381, 404, 878, 38, 442, 116, 900, 150, 72, 647, 632, 490, 930, 480, 226, 611, 656, 539, 168, 435, 793, 823, 847, 118, 99, 323, 156, 879, 288, 76, 294, 744, 163, 789, 898, 826, 556, 931, 832, 568, 809, 687, 829, 805, 947, 817, 877, 728, 28, 953, 725, 589, 530, 756, 132, 27, 580, 569, 464, 59, 527, 722, 718, 31, 838, 802, 449, 627, 249, 702, 326, 261, 692, 50, 211, 501, 221, 315, 342, 865, 677, 186, 618, 694, 207, 616, 526, 122, 110, 631, 262, 519, 462, 857, 378, 230, 220, 204, 895, 194, 421, 644, 512, 686, 100, 9, 950, 240, 864, 489, 552, 171, 396, 264, 662, 85, 500, 617, 310, 803, 792, 892, 311, 201, 165, 173, 112, 536, 614, 451, 384, 196, 245, 388, 331, 360, 139, 837, 678, 467, 159, 891, 203, 836, 914, 835, 77, 634, 759, 935, 596, 154, 402, 729, 424, 466, 659, 748, 945, 361, 476, 238, 668, 42, 439, 252, 82, 153, 902, 928, 597, 843, 422, 344, 621, 535, 509, 343, 693, 565, 521, 120, 633, 39, 776, 607, 604, 373, 544, 409, 551, 338, 401, 66, 293, 241, 511, 267, 314, 684, 514, 387, 63, 688, 62, 683, 882, 579, 785, 918, 518, 820, 363, 316, 452, 178, 73, 412, 870, 64, 532, 652, 237, 681, 841, 496, 246, 863, 571, 908, 601, 590, 899, 426, 944, 610, 905, 210, 416, 667, 912, 515, 406, 806, 679, 507, 751, 359, 834, 281, 890, 67, 784, 394, 615, 287, 816, 655, 904, 295, 74, 121, 12, 913, 791, 419, 587, 896, 675, 199, 840, 371, 800, 557, 894, 746, 313, 903, 58, 695, 102, 630, 167, 325, 269, 819, 657, 83, 830, 697, 612, 859, 949, 848, 84, 282, 405, 787, 769, 187, 516, 352, 887, 455, 812, 703, 873, 737, 456, 765, 609, 174, 773, 446, 750, 884, 762, 443, 350, 291, 180, 661, 780, 181, 382, 752, 34, 690, 303, 148, 355, 576, 430, 75, 825, 794, 743, 318, 712, 872, 268, 21, 665, 114, 289, 215, 277, 347, 16, 254, 44, 400, 761, 322, 850]\n",
      "\n",
      "Hay 479 Indices de Test:\n",
      "[540, 324, 92, 620, 934, 454, 727, 125, 440, 486, 458, 915, 321, 937, 69, 48, 172, 93, 745, 670, 206, 561, 635, 242, 741, 637, 390, 925, 861, 278, 948, 469, 831, 4, 523, 666, 734, 30, 370, 528, 275, 411, 256, 567, 885, 504, 591, 888, 709, 720, 111, 705, 170, 485, 470, 147, 305, 701, 8, 553, 7, 407, 599, 911, 669, 714, 29, 22, 89, 586, 593, 484, 782, 646, 946, 239, 808, 33, 654, 798, 329, 346, 625, 192, 354, 488, 0, 127, 939, 144, 244, 813, 603, 152, 308, 299, 224, 901, 115, 471, 691, 849, 349, 399, 513, 340, 860, 208, 560, 807, 391, 821, 232, 558, 482, 954, 758, 545, 434, 648, 164, 413, 68, 481, 778, 926, 623, 583, 123, 126, 107, 383, 166, 740, 529, 573, 105, 889, 188, 796, 91, 626, 779, 645, 731, 213, 460, 312, 375, 640, 613, 106, 376, 941, 707, 143, 534, 197, 43, 664, 54, 537, 328, 79, 525, 80, 498, 189, 710, 162, 417, 724, 185, 94, 35, 108, 36, 660, 345, 129, 428, 542, 176, 195, 478, 149, 270, 457, 799, 842, 222, 698, 582, 721, 708, 749, 772, 236, 263, 730, 429, 2, 566, 317, 533, 212, 65, 547, 598, 867, 581, 522, 881, 160, 570, 855, 763, 876, 735, 833, 846, 853, 234, 559, 845, 145, 719, 369, 57, 302, 372, 742, 389, 649, 736, 327, 233, 943, 223, 643, 357, 128, 151, 594, 886, 811, 193, 266, 871, 595, 415, 101, 726, 473, 492, 479, 764, 506, 140, 301, 563, 906, 161, 771, 739, 290, 919, 453, 783, 531, 788, 433, 951, 395, 356, 55, 40, 368, 543, 109, 459, 87, 658, 839, 60, 81, 307, 155, 592, 138, 874, 786, 696, 897, 642, 862, 554, 292, 546, 663, 247, 600, 468, 133, 713, 754, 441, 280, 555, 408, 770, 517, 795, 158, 51, 760, 17, 671, 248, 339, 562, 775, 14, 940, 11, 142, 638, 137, 757, 856, 32, 577, 463, 732, 253, 19, 414, 255, 619, 608, 335, 907, 936, 272, 715, 24, 20, 183, 198, 157, 425, 23, 641, 410, 952, 549, 818, 923, 52, 90, 258, 651, 300, 341, 550, 276, 438, 465, 450, 134, 379, 205, 397, 297, 78, 216, 362, 190, 393, 927, 781, 716, 348, 3, 797, 184, 472, 88, 70, 815, 733, 824, 285, 225, 924, 47, 578, 774, 854, 639, 273, 45, 477, 377, 717, 175, 723, 337, 202, 177, 332, 218, 880, 869, 858, 333, 15, 141, 296, 852, 279, 548, 494, 447, 229, 374, 214, 319, 606, 491, 564, 209, 933, 103, 320, 398, 932, 260, 767, 541, 432, 810, 673, 334, 365, 650, 136, 364, 538, 499, 624, 495, 306, 445, 403, 86, 921, 385, 6, 169, 706, 336, 71, 53, 584, 689, 61, 104, 636, 766, 251, 200, 822, 56, 672, 942, 699, 680, 41]\n",
      "##################################\n",
      "\n",
      "> Validacion Simple German\n",
      "Hay 500 indices de Train:\n",
      "[14, 775, 743, 688, 342, 61, 173, 27, 798, 602, 243, 332, 842, 606, 356, 457, 324, 996, 226, 815, 933, 797, 272, 541, 819, 804, 825, 231, 814, 127, 543, 129, 722, 516, 646, 397, 979, 693, 192, 207, 435, 886, 828, 507, 601, 93, 623, 458, 281, 851, 65, 952, 649, 871, 660, 229, 588, 935, 755, 437, 552, 605, 770, 752, 286, 385, 846, 529, 121, 859, 371, 652, 547, 751, 216, 417, 115, 262, 123, 835, 135, 451, 629, 30, 185, 786, 56, 199, 876, 643, 132, 536, 719, 907, 520, 253, 514, 549, 513, 592, 556, 230, 927, 509, 464, 96, 892, 826, 911, 733, 539, 958, 682, 7, 344, 485, 718, 228, 545, 999, 714, 998, 367, 503, 78, 381, 510, 692, 839, 816, 24, 70, 962, 597, 850, 505, 351, 642, 908, 615, 283, 865, 445, 125, 114, 300, 170, 278, 730, 477, 833, 289, 856, 645, 607, 651, 522, 890, 128, 948, 380, 422, 267, 200, 567, 894, 701, 616, 848, 427, 9, 395, 689, 335, 450, 232, 531, 916, 168, 726, 28, 880, 369, 583, 713, 74, 491, 401, 87, 574, 18, 44, 0, 750, 429, 80, 92, 257, 147, 490, 809, 665, 99, 571, 447, 117, 813, 98, 785, 148, 808, 684, 310, 749, 142, 466, 258, 658, 37, 413, 133, 25, 620, 81, 487, 982, 400, 91, 182, 191, 211, 626, 333, 869, 17, 741, 386, 64, 921, 570, 746, 152, 433, 517, 868, 113, 46, 793, 20, 997, 920, 474, 325, 208, 699, 681, 311, 838, 725, 675, 779, 150, 249, 576, 731, 330, 498, 218, 806, 76, 143, 164, 888, 702, 277, 495, 295, 430, 772, 650, 453, 205, 924, 569, 57, 496, 955, 768, 273, 59, 443, 354, 917, 915, 246, 512, 399, 252, 845, 932, 506, 654, 841, 664, 954, 368, 346, 739, 183, 339, 317, 298, 396, 337, 508, 349, 596, 640, 990, 341, 40, 84, 299, 566, 146, 632, 875, 233, 564, 625, 891, 863, 387, 735, 972, 723, 260, 55, 290, 110, 572, 171, 79, 340, 957, 100, 653, 634, 980, 959, 144, 613, 122, 716, 197, 967, 817, 279, 259, 163, 331, 235, 773, 610, 771, 767, 242, 794, 834, 116, 511, 362, 460, 348, 861, 608, 321, 462, 465, 742, 141, 870, 364, 138, 45, 266, 521, 854, 293, 287, 562, 604, 179, 619, 42, 655, 956, 153, 918, 831, 285, 881, 440, 347, 398, 805, 974, 177, 824, 248, 823, 438, 154, 910, 981, 885, 178, 365, 518, 818, 86, 987, 165, 48, 883, 622, 550, 97, 436, 678, 448, 584, 254, 43, 983, 301, 889, 237, 903, 112, 656, 598, 54, 250, 930, 844, 389, 119, 778, 255, 305, 155, 392, 618, 581, 10, 35, 214, 375, 836, 764, 973, 878, 674, 697, 636, 1, 829, 852, 638, 943, 363, 334, 593, 469, 446, 296, 533, 6, 580, 710, 727, 763, 190, 23, 29, 276, 378, 227, 630, 16, 288, 336, 758, 812, 648, 357, 906, 470, 315]\n",
      "\n",
      "Hay 500 Indices de Test:\n",
      "[196, 350, 431, 442, 975, 376, 294, 627, 708, 855, 225, 944, 558, 95, 769, 188, 705, 585, 738, 526, 366, 15, 573, 309, 103, 901, 377, 139, 176, 879, 432, 238, 542, 236, 412, 421, 853, 131, 563, 212, 175, 282, 704, 53, 239, 872, 676, 822, 744, 111, 799, 874, 461, 781, 206, 215, 902, 621, 75, 415, 641, 34, 223, 504, 274, 687, 403, 502, 318, 416, 101, 68, 280, 390, 936, 297, 525, 631, 753, 789, 51, 482, 158, 947, 909, 810, 712, 820, 711, 493, 391, 39, 219, 459, 969, 534, 162, 463, 159, 535, 69, 124, 994, 47, 706, 481, 220, 795, 780, 803, 926, 591, 546, 172, 862, 49, 970, 484, 261, 992, 698, 405, 388, 407, 492, 90, 234, 557, 425, 32, 578, 263, 670, 965, 659, 884, 11, 22, 538, 456, 66, 322, 210, 984, 736, 213, 167, 26, 644, 867, 370, 50, 847, 489, 700, 668, 449, 734, 268, 426, 134, 58, 898, 394, 671, 821, 373, 537, 717, 304, 934, 181, 320, 790, 467, 951, 12, 945, 575, 4, 709, 104, 308, 120, 669, 338, 327, 893, 85, 624, 553, 67, 985, 864, 203, 966, 323, 316, 424, 107, 792, 949, 52, 189, 882, 589, 672, 590, 359, 530, 217, 703, 161, 686, 404, 766, 41, 33, 776, 690, 737, 843, 540, 240, 151, 360, 2, 560, 102, 523, 914, 832, 939, 995, 195, 791, 475, 774, 83, 782, 419, 633, 19, 408, 740, 406, 783, 680, 126, 434, 423, 402, 544, 63, 548, 221, 551, 88, 352, 414, 193, 303, 71, 312, 186, 497, 204, 222, 904, 594, 896, 787, 180, 849, 938, 73, 720, 494, 694, 428, 777, 361, 319, 614, 418, 599, 919, 528, 595, 940, 732, 691, 420, 756, 565, 313, 454, 577, 784, 187, 661, 486, 8, 31, 679, 379, 265, 145, 241, 991, 343, 963, 554, 761, 244, 827, 38, 3, 657, 245, 473, 866, 149, 194, 807, 977, 105, 326, 559, 611, 800, 961, 897, 802, 860, 993, 637, 500, 929, 971, 600, 895, 499, 527, 483, 444, 696, 721, 383, 899, 729, 355, 728, 345, 302, 393, 715, 201, 264, 270, 912, 271, 472, 291, 579, 561, 515, 837, 157, 931, 224, 60, 468, 353, 617, 137, 209, 989, 603, 184, 136, 480, 586, 89, 858, 748, 372, 759, 760, 109, 384, 635, 140, 411, 765, 568, 166, 478, 374, 788, 754, 976, 928, 130, 960, 707, 409, 108, 455, 695, 647, 314, 663, 439, 724, 441, 887, 639, 21, 382, 747, 988, 762, 923, 609, 905, 796, 673, 532, 942, 306, 953, 247, 925, 745, 169, 174, 275, 307, 251, 950, 683, 410, 900, 118, 937, 198, 106, 62, 662, 612, 922, 82, 36, 452, 946, 5, 685, 830, 978, 986, 292, 666, 811, 94, 667, 519, 941, 877, 873, 582, 857, 628, 968, 269, 501, 284, 160, 476, 757, 328, 471, 587, 77, 677, 13, 358, 555, 913, 329, 488, 479, 202, 256, 156, 964, 72, 840, 524, 801]\n",
      "##################################\n",
      "Hay 500 indices de Train:\n",
      "[132, 21, 833, 760, 331, 785, 38, 73, 678, 295, 1, 757, 348, 66, 492, 396, 982, 170, 158, 418, 222, 140, 764, 221, 942, 629, 650, 5, 605, 890, 580, 133, 562, 717, 68, 296, 214, 500, 898, 67, 390, 955, 163, 648, 660, 203, 636, 525, 940, 287, 312, 229, 990, 720, 439, 339, 712, 417, 535, 90, 661, 773, 482, 399, 349, 401, 318, 283, 317, 481, 799, 903, 509, 279, 926, 218, 172, 332, 143, 57, 316, 71, 18, 989, 884, 542, 952, 840, 61, 394, 23, 343, 186, 254, 769, 702, 593, 536, 249, 303, 639, 895, 547, 657, 791, 765, 948, 591, 409, 76, 874, 753, 806, 465, 116, 528, 738, 767, 400, 159, 404, 0, 965, 447, 704, 850, 384, 632, 315, 871, 694, 173, 372, 668, 241, 184, 852, 635, 461, 608, 353, 153, 763, 188, 228, 148, 938, 200, 870, 564, 831, 862, 986, 893, 53, 597, 40, 478, 663, 485, 896, 375, 466, 768, 905, 402, 734, 584, 150, 796, 246, 976, 912, 599, 363, 751, 326, 624, 362, 824, 689, 267, 381, 484, 670, 407, 347, 587, 271, 682, 676, 855, 748, 108, 233, 84, 780, 305, 722, 963, 883, 644, 342, 262, 275, 522, 869, 387, 545, 789, 309, 39, 8, 888, 899, 823, 915, 236, 793, 502, 607, 98, 272, 54, 92, 917, 960, 20, 248, 75, 386, 297, 699, 936, 741, 33, 161, 144, 516, 199, 830, 397, 364, 436, 35, 109, 82, 784, 220, 345, 463, 601, 240, 460, 625, 586, 875, 266, 388, 48, 521, 87, 15, 112, 395, 651, 350, 559, 428, 935, 410, 787, 941, 327, 779, 911, 255, 630, 49, 878, 508, 252, 807, 487, 298, 446, 854, 291, 974, 376, 724, 462, 700, 123, 782, 801, 643, 742, 145, 442, 290, 336, 759, 9, 711, 931, 654, 370, 802, 285, 411, 579, 713, 426, 337, 138, 37, 904, 746, 338, 993, 667, 691, 351, 42, 385, 10, 697, 589, 640, 237, 286, 224, 970, 583, 113, 311, 341, 334, 795, 81, 872, 55, 22, 641, 422, 719, 232, 695, 638, 441, 141, 662, 642, 656, 456, 34, 17, 36, 592, 424, 494, 205, 281, 263, 664, 815, 299, 94, 65, 518, 320, 392, 314, 679, 610, 961, 77, 631, 817, 44, 964, 612, 211, 743, 234, 174, 958, 437, 556, 929, 176, 950, 62, 448, 877, 933, 443, 876, 293, 78, 165, 358, 809, 491, 687, 739, 83, 550, 373, 848, 541, 480, 666, 810, 510, 526, 987, 95, 183, 357, 458, 889, 538, 603, 843, 621, 368, 723, 434, 243, 726, 269, 728, 307, 731, 310, 577, 328, 230, 897, 335, 887, 581, 825, 819, 776, 432, 171, 406, 43, 321, 180, 104, 268, 300, 64, 957, 14, 837, 790, 966, 934, 745, 304, 344, 93, 470, 919, 529, 949, 788, 103, 72, 70, 427, 568, 669, 920, 606, 946, 464, 747, 727, 698, 705, 546, 544, 749, 365, 313, 121, 962, 569, 136, 971, 994, 419, 886, 253]\n",
      "\n",
      "Hay 500 Indices de Test:\n",
      "[382, 216, 812, 730, 498, 703, 557, 181, 658, 134, 814, 752, 215, 235, 945, 51, 352, 555, 149, 999, 7, 891, 292, 975, 453, 80, 735, 930, 841, 139, 495, 845, 696, 202, 615, 859, 686, 289, 927, 693, 405, 277, 922, 333, 820, 832, 574, 403, 391, 853, 118, 972, 527, 301, 306, 294, 659, 708, 524, 219, 473, 47, 847, 783, 754, 762, 867, 880, 440, 26, 652, 857, 450, 901, 360, 429, 573, 25, 706, 129, 126, 259, 91, 775, 369, 900, 590, 804, 637, 169, 744, 414, 457, 879, 858, 540, 191, 250, 308, 616, 634, 86, 378, 984, 206, 617, 563, 13, 609, 479, 836, 398, 892, 932, 813, 721, 548, 943, 967, 380, 4, 59, 16, 483, 925, 575, 58, 60, 201, 209, 576, 280, 114, 766, 475, 472, 623, 939, 416, 45, 554, 585, 997, 622, 829, 194, 504, 626, 861, 552, 179, 127, 937, 671, 822, 444, 882, 551, 100, 774, 182, 302, 908, 781, 794, 907, 111, 980, 408, 247, 842, 154, 618, 168, 30, 537, 99, 868, 276, 430, 598, 282, 486, 155, 431, 797, 346, 208, 881, 501, 846, 655, 157, 24, 50, 274, 142, 162, 323, 611, 242, 261, 688, 468, 452, 238, 998, 956, 792, 718, 560, 680, 190, 808, 681, 978, 451, 944, 354, 489, 909, 257, 756, 19, 758, 156, 147, 152, 512, 520, 421, 553, 499, 951, 488, 27, 119, 79, 374, 910, 725, 435, 371, 151, 210, 733, 864, 507, 107, 28, 977, 985, 531, 786, 561, 245, 212, 750, 503, 645, 838, 196, 619, 415, 690, 324, 595, 490, 137, 602, 800, 31, 740, 571, 476, 594, 684, 41, 477, 649, 12, 921, 685, 258, 683, 178, 992, 122, 412, 844, 860, 600, 996, 692, 356, 124, 866, 913, 225, 772, 413, 420, 973, 534, 885, 213, 714, 816, 120, 572, 928, 778, 32, 260, 894, 924, 865, 389, 115, 164, 873, 231, 29, 101, 367, 827, 947, 459, 264, 319, 433, 914, 469, 969, 166, 646, 359, 130, 770, 177, 981, 777, 995, 505, 515, 530, 455, 818, 284, 803, 716, 471, 729, 523, 366, 570, 835, 197, 856, 244, 102, 567, 539, 701, 195, 125, 710, 532, 968, 85, 131, 393, 265, 863, 578, 97, 474, 979, 566, 454, 628, 627, 834, 533, 3, 175, 6, 565, 69, 193, 771, 187, 675, 953, 379, 204, 709, 11, 558, 543, 493, 355, 991, 256, 506, 983, 128, 849, 732, 519, 647, 916, 74, 988, 288, 582, 828, 2, 902, 811, 325, 954, 588, 906, 449, 383, 110, 438, 425, 146, 549, 227, 467, 707, 613, 755, 826, 496, 805, 56, 511, 715, 673, 736, 497, 677, 839, 672, 322, 377, 167, 959, 517, 665, 445, 761, 329, 251, 46, 89, 270, 596, 106, 207, 217, 798, 674, 273, 160, 604, 189, 737, 117, 192, 633, 923, 52, 361, 63, 513, 821, 278, 514, 198, 96, 423, 851, 620, 653, 239, 330, 614, 105, 226, 918, 223, 135, 340, 185, 88]\n",
      "##################################\n",
      "\n",
      "> Validacion Cruzada Tic-Tac-Toe\n",
      "Hay 638 indices de Train:\n",
      "[83, 299, 388, 53, 704, 568, 331, 672, 502, 210, 366, 143, 155, 842, 243, 456, 501, 298, 647, 642, 141, 876, 762, 175, 68, 746, 241, 540, 232, 177, 444, 91, 340, 614, 471, 572, 248, 3, 265, 563, 944, 741, 228, 900, 670, 470, 953, 258, 582, 687, 382, 877, 701, 230, 690, 829, 683, 725, 345, 48, 624, 954, 379, 774, 807, 113, 822, 397, 180, 413, 371, 90, 81, 765, 736, 225, 151, 870, 813, 847, 500, 883, 385, 206, 307, 286, 956, 326, 825, 224, 13, 770, 904, 663, 188, 680, 865, 628, 220, 406, 67, 275, 435, 6, 858, 873, 49, 576, 685, 288, 518, 602, 926, 695, 833, 167, 782, 0, 897, 247, 400, 845, 947, 459, 305, 912, 73, 880, 441, 306, 355, 407, 684, 457, 567, 927, 562, 128, 513, 745, 903, 372, 661, 257, 627, 353, 777, 383, 739, 209, 625, 653, 535, 617, 78, 848, 919, 806, 846, 931, 634, 907, 951, 291, 946, 358, 463, 579, 698, 127, 47, 818, 805, 106, 201, 76, 8, 952, 760, 854, 955, 302, 112, 405, 524, 1, 11, 296, 851, 200, 55, 115, 93, 816, 766, 198, 933, 169, 729, 59, 485, 601, 26, 553, 872, 108, 530, 344, 477, 139, 250, 240, 132, 677, 54, 260, 584, 133, 194, 678, 713, 79, 497, 66, 437, 304, 122, 794, 660, 216, 465, 386, 430, 651, 478, 778, 849, 688, 100, 840, 374, 439, 404, 613, 474, 756, 885, 293, 503, 656, 532, 612, 178, 438, 165, 464, 354, 264, 476, 33, 504, 587, 630, 71, 936, 37, 12, 57, 28, 270, 428, 886, 905, 259, 603, 45, 320, 85, 317, 104, 82, 313, 707, 80, 529, 626, 923, 868, 772, 324, 506, 381, 785, 369, 691, 327, 215, 699, 117, 368, 423, 832, 7, 943, 561, 329, 860, 781, 440, 649, 138, 605, 852, 479, 205, 668, 312, 152, 300, 763, 447, 65, 793, 31, 636, 86, 495, 577, 821, 219, 788, 859, 780, 410, 856, 738, 728, 294, 52, 839, 409, 483, 203, 508, 727, 97, 114, 481, 285, 199, 574, 585, 930, 395, 310, 641, 375, 945, 743, 556, 475, 749, 431, 136, 161, 499, 396, 874, 359, 325, 957, 828, 857, 77, 827, 586, 231, 648, 391, 566, 336, 939, 420, 62, 732, 523, 429, 171, 461, 269, 715, 531, 837, 19, 514, 72, 735, 797, 32, 871, 511, 796, 213, 217, 14, 838, 519, 560, 454, 21, 545, 490, 207, 398, 863, 769, 9, 755, 153, 158, 357, 679, 604, 356, 768, 34, 251, 452, 323, 418, 869, 757, 157, 557, 855, 424, 392, 125, 212, 256, 349, 881, 208, 342, 934, 726, 843, 615, 414, 64, 682, 197, 814, 803, 234, 812, 56, 547, 273, 665, 101, 948, 775, 853, 387, 706, 526, 826, 38, 272, 515, 841, 790, 211, 635, 709, 542, 236, 591, 861, 696, 711, 134, 16, 808, 35, 164, 172, 338, 667, 249, 941, 69, 309, 487, 84, 70, 929, 505, 195, 427, 702, 252, 789, 831, 271, 466, 657, 509, 830, 10, 279, 942, 631, 244, 23, 337, 565, 507, 20, 705, 536, 120, 181, 191, 595, 5, 884, 570, 253, 346, 130, 319, 676, 811, 662, 554, 693, 186, 588, 879, 144, 525, 875, 623, 935, 744, 915, 267, 455, 599, 692, 448, 412, 650, 187, 121, 166, 610, 42, 950, 63, 239, 411, 646, 408, 703, 510, 227, 223, 432, 792, 798, 419, 708, 399, 266, 899, 335, 742, 255, 795, 597, 844, 280, 29, 590, 609, 482, 917, 527, 783, 116, 60, 546, 99, 918, 105, 176, 154, 629, 643, 416, 882, 717, 800, 712, 472, 911, 809, 278, 489, 921, 887, 655, 218, 314, 129, 303, 559, 710, 916, 940, 521, 146, 168, 393, 364, 402, 87, 147, 639, 492, 733, 633, 401, 235, 608]\n",
      "\n",
      "Hay 319 Indices de Test:\n",
      "[543, 360, 767, 292, 322, 598, 569, 102, 714, 517, 367, 817, 46, 664, 95, 669, 611, 274, 866, 370, 724, 268, 776, 819, 551, 377, 836, 88, 949, 581, 541, 159, 824, 787, 596, 654, 516, 888, 51, 380, 290, 640, 174, 906, 61, 719, 761, 555, 433, 384, 425, 659, 784, 442, 362, 170, 619, 237, 773, 674, 754, 666, 644, 156, 734, 898, 606, 520, 467, 720, 202, 908, 488, 494, 512, 697, 238, 242, 89, 480, 92, 103, 376, 764, 622, 287, 902, 528, 550, 564, 901, 867, 426, 378, 893, 593, 681, 575, 753, 446, 533, 583, 119, 436, 330, 328, 173, 347, 297, 700, 638, 652, 496, 222, 632, 671, 637, 262, 361, 552, 820, 30, 469, 621, 895, 422, 333, 348, 365, 689, 351, 486, 39, 894, 43, 363, 548, 747, 834, 462, 373, 111, 160, 276, 193, 334, 534, 98, 600, 96, 445, 937, 44, 17, 571, 233, 799, 277, 549, 192, 804, 229, 150, 580, 196, 40, 913, 864, 620, 538, 779, 449, 283, 607, 810, 925, 301, 179, 394, 752, 94, 41, 245, 618, 740, 823, 815, 891, 263, 453, 491, 4, 214, 835, 190, 254, 594, 126, 137, 226, 50, 184, 417, 123, 473, 716, 786, 771, 878, 163, 932, 484, 295, 204, 544, 451, 24, 802, 343, 145, 460, 403, 675, 673, 616, 558, 282, 316, 246, 748, 58, 162, 862, 183, 750, 493, 791, 924, 458, 498, 896, 339, 182, 434, 261, 578, 389, 537, 2, 36, 589, 658, 149, 284, 189, 751, 281, 914, 922, 890, 592, 341, 718, 109, 142, 25, 308, 107, 135, 350, 910, 311, 140, 110, 759, 522, 758, 352, 450, 185, 421, 315, 148, 443, 390, 938, 221, 415, 645, 801, 731, 332, 321, 539, 22, 131, 892, 27, 468, 920, 889, 74, 15, 909, 318, 730, 289, 686, 850, 573, 723, 118, 737, 928, 124, 18, 75, 721, 694]\n",
      "##################################\n",
      "Hay 638 indices de Train:\n",
      "[543, 360, 767, 292, 322, 598, 569, 102, 714, 517, 367, 817, 46, 664, 95, 669, 611, 274, 866, 370, 724, 268, 776, 819, 551, 377, 836, 88, 949, 581, 541, 159, 824, 787, 596, 654, 516, 888, 51, 380, 290, 640, 174, 906, 61, 719, 761, 555, 433, 384, 425, 659, 784, 442, 362, 170, 619, 237, 773, 674, 754, 666, 644, 156, 734, 898, 606, 520, 467, 720, 202, 908, 488, 494, 512, 697, 238, 242, 89, 480, 92, 103, 376, 764, 622, 287, 902, 528, 550, 564, 901, 867, 426, 378, 893, 593, 681, 575, 753, 446, 533, 583, 119, 436, 330, 328, 173, 347, 297, 700, 638, 652, 496, 222, 632, 671, 637, 262, 361, 552, 820, 30, 469, 621, 895, 422, 333, 348, 365, 689, 351, 486, 39, 894, 43, 363, 548, 747, 834, 462, 373, 111, 160, 276, 193, 334, 534, 98, 600, 96, 445, 937, 44, 17, 571, 233, 799, 277, 549, 192, 804, 229, 150, 580, 196, 40, 913, 864, 620, 538, 779, 449, 283, 607, 810, 925, 301, 179, 394, 752, 94, 41, 245, 618, 740, 823, 815, 891, 263, 453, 491, 4, 214, 835, 190, 254, 594, 126, 137, 226, 50, 184, 417, 123, 473, 716, 786, 771, 878, 163, 932, 484, 295, 204, 544, 451, 24, 802, 343, 145, 460, 403, 675, 673, 616, 558, 282, 316, 246, 748, 58, 162, 862, 183, 750, 493, 791, 924, 458, 498, 896, 339, 182, 434, 261, 578, 389, 537, 2, 36, 589, 658, 149, 284, 189, 751, 281, 914, 922, 890, 592, 341, 718, 109, 142, 25, 308, 107, 135, 350, 910, 311, 140, 110, 759, 522, 758, 352, 450, 185, 421, 315, 148, 443, 390, 938, 221, 415, 645, 801, 731, 332, 321, 539, 22, 131, 892, 27, 468, 920, 889, 74, 15, 909, 318, 730, 289, 686, 850, 573, 723, 118, 737, 928, 124, 18, 75, 721, 694, 763, 447, 65, 793, 31, 636, 86, 495, 577, 821, 219, 788, 859, 780, 410, 856, 738, 728, 294, 52, 839, 409, 483, 203, 508, 727, 97, 114, 481, 285, 199, 574, 585, 930, 395, 310, 641, 375, 945, 743, 556, 475, 749, 431, 136, 161, 499, 396, 874, 359, 325, 957, 828, 857, 77, 827, 586, 231, 648, 391, 566, 336, 939, 420, 62, 732, 523, 429, 171, 461, 269, 715, 531, 837, 19, 514, 72, 735, 797, 32, 871, 511, 796, 213, 217, 14, 838, 519, 560, 454, 21, 545, 490, 207, 398, 863, 769, 9, 755, 153, 158, 357, 679, 604, 356, 768, 34, 251, 452, 323, 418, 869, 757, 157, 557, 855, 424, 392, 125, 212, 256, 349, 881, 208, 342, 934, 726, 843, 615, 414, 64, 682, 197, 814, 803, 234, 812, 56, 547, 273, 665, 101, 948, 775, 853, 387, 706, 526, 826, 38, 272, 515, 841, 790, 211, 635, 709, 542, 236, 591, 861, 696, 711, 134, 16, 808, 35, 164, 172, 338, 667, 249, 941, 69, 309, 487, 84, 70, 929, 505, 195, 427, 702, 252, 789, 831, 271, 466, 657, 509, 830, 10, 279, 942, 631, 244, 23, 337, 565, 507, 20, 705, 536, 120, 181, 191, 595, 5, 884, 570, 253, 346, 130, 319, 676, 811, 662, 554, 693, 186, 588, 879, 144, 525, 875, 623, 935, 744, 915, 267, 455, 599, 692, 448, 412, 650, 187, 121, 166, 610, 42, 950, 63, 239, 411, 646, 408, 703, 510, 227, 223, 432, 792, 798, 419, 708, 399, 266, 899, 335, 742, 255, 795, 597, 844, 280, 29, 590, 609, 482, 917, 527, 783, 116, 60, 546, 99, 918, 105, 176, 154, 629, 643, 416, 882, 717, 800, 712, 472, 911, 809, 278, 489, 921, 887, 655, 218, 314, 129, 303, 559, 710, 916, 940, 521, 146, 168, 393, 364, 402, 87, 147, 639, 492, 733, 633, 401, 235, 608]\n",
      "\n",
      "Hay 319 Indices de Test:\n",
      "[83, 299, 388, 53, 704, 568, 331, 672, 502, 210, 366, 143, 155, 842, 243, 456, 501, 298, 647, 642, 141, 876, 762, 175, 68, 746, 241, 540, 232, 177, 444, 91, 340, 614, 471, 572, 248, 3, 265, 563, 944, 741, 228, 900, 670, 470, 953, 258, 582, 687, 382, 877, 701, 230, 690, 829, 683, 725, 345, 48, 624, 954, 379, 774, 807, 113, 822, 397, 180, 413, 371, 90, 81, 765, 736, 225, 151, 870, 813, 847, 500, 883, 385, 206, 307, 286, 956, 326, 825, 224, 13, 770, 904, 663, 188, 680, 865, 628, 220, 406, 67, 275, 435, 6, 858, 873, 49, 576, 685, 288, 518, 602, 926, 695, 833, 167, 782, 0, 897, 247, 400, 845, 947, 459, 305, 912, 73, 880, 441, 306, 355, 407, 684, 457, 567, 927, 562, 128, 513, 745, 903, 372, 661, 257, 627, 353, 777, 383, 739, 209, 625, 653, 535, 617, 78, 848, 919, 806, 846, 931, 634, 907, 951, 291, 946, 358, 463, 579, 698, 127, 47, 818, 805, 106, 201, 76, 8, 952, 760, 854, 955, 302, 112, 405, 524, 1, 11, 296, 851, 200, 55, 115, 93, 816, 766, 198, 933, 169, 729, 59, 485, 601, 26, 553, 872, 108, 530, 344, 477, 139, 250, 240, 132, 677, 54, 260, 584, 133, 194, 678, 713, 79, 497, 66, 437, 304, 122, 794, 660, 216, 465, 386, 430, 651, 478, 778, 849, 688, 100, 840, 374, 439, 404, 613, 474, 756, 885, 293, 503, 656, 532, 612, 178, 438, 165, 464, 354, 264, 476, 33, 504, 587, 630, 71, 936, 37, 12, 57, 28, 270, 428, 886, 905, 259, 603, 45, 320, 85, 317, 104, 82, 313, 707, 80, 529, 626, 923, 868, 772, 324, 506, 381, 785, 369, 691, 327, 215, 699, 117, 368, 423, 832, 7, 943, 561, 329, 860, 781, 440, 649, 138, 605, 852, 479, 205, 668, 312, 152, 300]\n",
      "##################################\n",
      "Hay 638 indices de Train:\n",
      "[543, 360, 767, 292, 322, 598, 569, 102, 714, 517, 367, 817, 46, 664, 95, 669, 611, 274, 866, 370, 724, 268, 776, 819, 551, 377, 836, 88, 949, 581, 541, 159, 824, 787, 596, 654, 516, 888, 51, 380, 290, 640, 174, 906, 61, 719, 761, 555, 433, 384, 425, 659, 784, 442, 362, 170, 619, 237, 773, 674, 754, 666, 644, 156, 734, 898, 606, 520, 467, 720, 202, 908, 488, 494, 512, 697, 238, 242, 89, 480, 92, 103, 376, 764, 622, 287, 902, 528, 550, 564, 901, 867, 426, 378, 893, 593, 681, 575, 753, 446, 533, 583, 119, 436, 330, 328, 173, 347, 297, 700, 638, 652, 496, 222, 632, 671, 637, 262, 361, 552, 820, 30, 469, 621, 895, 422, 333, 348, 365, 689, 351, 486, 39, 894, 43, 363, 548, 747, 834, 462, 373, 111, 160, 276, 193, 334, 534, 98, 600, 96, 445, 937, 44, 17, 571, 233, 799, 277, 549, 192, 804, 229, 150, 580, 196, 40, 913, 864, 620, 538, 779, 449, 283, 607, 810, 925, 301, 179, 394, 752, 94, 41, 245, 618, 740, 823, 815, 891, 263, 453, 491, 4, 214, 835, 190, 254, 594, 126, 137, 226, 50, 184, 417, 123, 473, 716, 786, 771, 878, 163, 932, 484, 295, 204, 544, 451, 24, 802, 343, 145, 460, 403, 675, 673, 616, 558, 282, 316, 246, 748, 58, 162, 862, 183, 750, 493, 791, 924, 458, 498, 896, 339, 182, 434, 261, 578, 389, 537, 2, 36, 589, 658, 149, 284, 189, 751, 281, 914, 922, 890, 592, 341, 718, 109, 142, 25, 308, 107, 135, 350, 910, 311, 140, 110, 759, 522, 758, 352, 450, 185, 421, 315, 148, 443, 390, 938, 221, 415, 645, 801, 731, 332, 321, 539, 22, 131, 892, 27, 468, 920, 889, 74, 15, 909, 318, 730, 289, 686, 850, 573, 723, 118, 737, 928, 124, 18, 75, 721, 694, 83, 299, 388, 53, 704, 568, 331, 672, 502, 210, 366, 143, 155, 842, 243, 456, 501, 298, 647, 642, 141, 876, 762, 175, 68, 746, 241, 540, 232, 177, 444, 91, 340, 614, 471, 572, 248, 3, 265, 563, 944, 741, 228, 900, 670, 470, 953, 258, 582, 687, 382, 877, 701, 230, 690, 829, 683, 725, 345, 48, 624, 954, 379, 774, 807, 113, 822, 397, 180, 413, 371, 90, 81, 765, 736, 225, 151, 870, 813, 847, 500, 883, 385, 206, 307, 286, 956, 326, 825, 224, 13, 770, 904, 663, 188, 680, 865, 628, 220, 406, 67, 275, 435, 6, 858, 873, 49, 576, 685, 288, 518, 602, 926, 695, 833, 167, 782, 0, 897, 247, 400, 845, 947, 459, 305, 912, 73, 880, 441, 306, 355, 407, 684, 457, 567, 927, 562, 128, 513, 745, 903, 372, 661, 257, 627, 353, 777, 383, 739, 209, 625, 653, 535, 617, 78, 848, 919, 806, 846, 931, 634, 907, 951, 291, 946, 358, 463, 579, 698, 127, 47, 818, 805, 106, 201, 76, 8, 952, 760, 854, 955, 302, 112, 405, 524, 1, 11, 296, 851, 200, 55, 115, 93, 816, 766, 198, 933, 169, 729, 59, 485, 601, 26, 553, 872, 108, 530, 344, 477, 139, 250, 240, 132, 677, 54, 260, 584, 133, 194, 678, 713, 79, 497, 66, 437, 304, 122, 794, 660, 216, 465, 386, 430, 651, 478, 778, 849, 688, 100, 840, 374, 439, 404, 613, 474, 756, 885, 293, 503, 656, 532, 612, 178, 438, 165, 464, 354, 264, 476, 33, 504, 587, 630, 71, 936, 37, 12, 57, 28, 270, 428, 886, 905, 259, 603, 45, 320, 85, 317, 104, 82, 313, 707, 80, 529, 626, 923, 868, 772, 324, 506, 381, 785, 369, 691, 327, 215, 699, 117, 368, 423, 832, 7, 943, 561, 329, 860, 781, 440, 649, 138, 605, 852, 479, 205, 668, 312, 152, 300]\n",
      "\n",
      "Hay 319 Indices de Test:\n",
      "[763, 447, 65, 793, 31, 636, 86, 495, 577, 821, 219, 788, 859, 780, 410, 856, 738, 728, 294, 52, 839, 409, 483, 203, 508, 727, 97, 114, 481, 285, 199, 574, 585, 930, 395, 310, 641, 375, 945, 743, 556, 475, 749, 431, 136, 161, 499, 396, 874, 359, 325, 957, 828, 857, 77, 827, 586, 231, 648, 391, 566, 336, 939, 420, 62, 732, 523, 429, 171, 461, 269, 715, 531, 837, 19, 514, 72, 735, 797, 32, 871, 511, 796, 213, 217, 14, 838, 519, 560, 454, 21, 545, 490, 207, 398, 863, 769, 9, 755, 153, 158, 357, 679, 604, 356, 768, 34, 251, 452, 323, 418, 869, 757, 157, 557, 855, 424, 392, 125, 212, 256, 349, 881, 208, 342, 934, 726, 843, 615, 414, 64, 682, 197, 814, 803, 234, 812, 56, 547, 273, 665, 101, 948, 775, 853, 387, 706, 526, 826, 38, 272, 515, 841, 790, 211, 635, 709, 542, 236, 591, 861, 696, 711, 134, 16, 808, 35, 164, 172, 338, 667, 249, 941, 69, 309, 487, 84, 70, 929, 505, 195, 427, 702, 252, 789, 831, 271, 466, 657, 509, 830, 10, 279, 942, 631, 244, 23, 337, 565, 507, 20, 705, 536, 120, 181, 191, 595, 5, 884, 570, 253, 346, 130, 319, 676, 811, 662, 554, 693, 186, 588, 879, 144, 525, 875, 623, 935, 744, 915, 267, 455, 599, 692, 448, 412, 650, 187, 121, 166, 610, 42, 950, 63, 239, 411, 646, 408, 703, 510, 227, 223, 432, 792, 798, 419, 708, 399, 266, 899, 335, 742, 255, 795, 597, 844, 280, 29, 590, 609, 482, 917, 527, 783, 116, 60, 546, 99, 918, 105, 176, 154, 629, 643, 416, 882, 717, 800, 712, 472, 911, 809, 278, 489, 921, 887, 655, 218, 314, 129, 303, 559, 710, 916, 940, 521, 146, 168, 393, 364, 402, 87, 147, 639, 492, 733, 633, 401, 235, 608]\n",
      "##################################\n",
      "\n",
      "> Validacion Cruzada German\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 666 indices de Train:\n",
      "[543, 975, 411, 845, 9, 744, 19, 768, 977, 628, 489, 684, 268, 698, 15, 759, 110, 795, 707, 855, 840, 107, 264, 889, 892, 580, 709, 681, 315, 185, 106, 651, 597, 227, 544, 710, 982, 1, 680, 482, 878, 775, 496, 546, 660, 662, 78, 221, 925, 675, 514, 655, 265, 996, 278, 485, 200, 833, 182, 593, 600, 339, 266, 449, 366, 603, 963, 545, 406, 789, 969, 450, 812, 388, 369, 257, 226, 115, 83, 95, 967, 819, 287, 552, 363, 235, 426, 147, 571, 33, 633, 602, 91, 735, 466, 497, 741, 618, 866, 275, 837, 624, 890, 585, 835, 104, 51, 865, 617, 492, 72, 541, 276, 639, 973, 808, 605, 386, 737, 342, 738, 673, 567, 422, 401, 30, 961, 443, 503, 14, 742, 320, 112, 765, 942, 130, 701, 414, 469, 407, 909, 210, 122, 81, 526, 402, 887, 873, 938, 472, 706, 674, 619, 745, 35, 688, 501, 590, 831, 936, 261, 217, 429, 670, 346, 607, 378, 376, 209, 156, 551, 199, 728, 827, 180, 964, 565, 263, 685, 682, 648, 898, 253, 729, 384, 805, 417, 303, 415, 601, 325, 858, 870, 566, 243, 629, 479, 478, 377, 242, 772, 806, 853, 904, 28, 314, 442, 288, 330, 720, 650, 488, 703, 368, 920, 131, 934, 419, 240, 365, 854, 994, 477, 612, 322, 258, 298, 154, 609, 810, 329, 783, 215, 408, 968, 56, 70, 349, 705, 777, 99, 537, 711, 721, 234, 616, 995, 750, 815, 656, 826, 454, 351, 216, 299, 916, 117, 644, 383, 326, 696, 770, 653, 493, 192, 632, 592, 371, 357, 594, 901, 20, 313, 412, 525, 43, 125, 823, 260, 284, 153, 259, 452, 713, 295, 471, 11, 385, 510, 135, 683, 816, 102, 608, 10, 972, 992, 625, 394, 61, 829, 321, 570, 203, 799, 86, 127, 395, 281, 793, 381, 87, 638, 76, 947, 869, 579, 962, 577, 524, 374, 906, 908, 610, 481, 956, 536, 586, 576, 42, 296, 621, 465, 247, 428, 148, 487, 876, 103, 740, 767, 116, 715, 277, 120, 241, 327, 877, 635, 989, 486, 355, 57, 811, 672, 344, 825, 290, 714, 136, 521, 392, 751, 175, 451, 548, 119, 188, 784, 172, 902, 177, 13, 504, 324, 678, 697, 262, 679, 304, 250, 646, 311, 74, 207, 132, 807, 931, 733, 317, 438, 924, 895, 842, 687, 766, 694, 224, 879, 181, 161, 310, 393, 642, 955, 484, 170, 933, 993, 588, 220, 397, 959, 724, 370, 205, 7, 29, 361, 228, 16, 239, 436, 425, 300, 251, 159, 212, 676, 309, 48, 47, 138, 462, 911, 549, 114, 554, 60, 64, 928, 133, 794, 400, 763, 722, 981, 145, 582, 867, 340, 93, 623, 900, 529, 630, 950, 448, 506, 923, 409, 160, 463, 430, 519, 637, 63, 606, 695, 622, 834, 294, 921, 4, 556, 953, 575, 21, 748, 123, 926, 944, 5, 464, 760, 184, 273, 756, 495, 445, 778, 218, 336, 500, 213, 780, 669, 316, 59, 399, 843, 68, 641, 187, 654, 561, 245, 434, 986, 757, 970, 231, 919, 828, 945, 587, 328, 954, 859, 998, 801, 169, 712, 591, 584, 896, 572, 764, 773, 547, 517, 976, 62, 677, 848, 0, 540, 347, 627, 164, 702, 915, 459, 813, 73, 899, 312, 564, 283, 174, 201, 912, 142, 851, 146, 719, 3, 987, 832, 531, 693, 439, 663, 985, 864, 129, 405, 350, 664, 282, 179, 230, 558, 458, 984, 255, 427, 338, 838, 671, 44, 754, 613, 53, 468, 372, 46, 31, 881, 522, 990, 747, 800, 581, 824, 467, 280, 421, 743, 17, 758, 398, 535, 516, 502, 948, 542, 331, 509, 178, 978, 305, 208, 413, 483, 348, 988, 538, 513, 802, 183, 18, 211, 557, 839, 596, 830, 817, 634, 94, 150, 65, 222, 89, 39, 128, 595, 318, 40, 652, 804, 437, 440, 306, 821, 269, 389, 232, 105, 12, 910, 917, 353, 559, 424, 249, 158, 927, 460, 50, 432, 534, 649, 846, 270, 176]\n",
      "\n",
      "Hay 333 Indices de Test:\n",
      "[857, 929, 75, 974, 746, 971, 643, 71, 204, 25, 893, 856, 598, 308, 137, 475, 491, 691, 168, 111, 461, 749, 319, 302, 456, 528, 573, 991, 196, 36, 717, 155, 49, 334, 358, 32, 152, 193, 229, 949, 762, 645, 286, 560, 803, 237, 423, 337, 518, 79, 875, 640, 882, 58, 941, 335, 913, 569, 769, 530, 362, 665, 252, 797, 700, 480, 404, 191, 884, 960, 214, 359, 792, 520, 323, 647, 37, 820, 886, 97, 957, 862, 163, 686, 667, 390, 966, 410, 849, 375, 233, 771, 307, 752, 67, 636, 186, 620, 891, 774, 626, 905, 498, 206, 999, 704, 420, 343, 171, 380, 699, 165, 818, 23, 301, 786, 515, 943, 126, 143, 219, 583, 6, 66, 658, 952, 121, 38, 84, 930, 814, 880, 197, 604, 289, 946, 433, 223, 82, 615, 666, 100, 550, 473, 589, 34, 189, 162, 555, 134, 267, 80, 785, 173, 734, 435, 809, 198, 505, 578, 474, 373, 124, 568, 202, 85, 292, 140, 739, 22, 367, 732, 983, 897, 761, 297, 431, 109, 631, 776, 396, 841, 539, 796, 27, 352, 731, 782, 533, 194, 101, 96, 718, 997, 453, 291, 791, 861, 736, 914, 455, 980, 248, 508, 512, 965, 689, 599, 354, 727, 457, 92, 141, 553, 2, 113, 139, 523, 563, 45, 781, 285, 847, 379, 236, 668, 935, 798, 940, 860, 872, 726, 716, 844, 446, 661, 562, 730, 333, 614, 149, 885, 836, 341, 708, 279, 293, 958, 166, 447, 723, 922, 272, 157, 90, 790, 476, 26, 345, 937, 246, 387, 932, 511, 144, 753, 788, 951, 24, 167, 574, 939, 364, 360, 8, 41, 274, 690, 852, 98, 190, 332, 903, 52, 868, 822, 499, 55, 356, 863, 894, 883, 418, 88, 918, 254, 532, 470, 391, 441, 692, 416, 611, 77, 256, 779, 494, 871, 151, 659, 118, 490, 225, 725, 108, 874, 527, 507, 444, 755, 382, 238, 54, 979, 657, 69, 403, 244, 907, 195, 850, 888, 787]\n",
      "##################################\n",
      "Hay 666 indices de Train:\n",
      "[857, 929, 75, 974, 746, 971, 643, 71, 204, 25, 893, 856, 598, 308, 137, 475, 491, 691, 168, 111, 461, 749, 319, 302, 456, 528, 573, 991, 196, 36, 717, 155, 49, 334, 358, 32, 152, 193, 229, 949, 762, 645, 286, 560, 803, 237, 423, 337, 518, 79, 875, 640, 882, 58, 941, 335, 913, 569, 769, 530, 362, 665, 252, 797, 700, 480, 404, 191, 884, 960, 214, 359, 792, 520, 323, 647, 37, 820, 886, 97, 957, 862, 163, 686, 667, 390, 966, 410, 849, 375, 233, 771, 307, 752, 67, 636, 186, 620, 891, 774, 626, 905, 498, 206, 999, 704, 420, 343, 171, 380, 699, 165, 818, 23, 301, 786, 515, 943, 126, 143, 219, 583, 6, 66, 658, 952, 121, 38, 84, 930, 814, 880, 197, 604, 289, 946, 433, 223, 82, 615, 666, 100, 550, 473, 589, 34, 189, 162, 555, 134, 267, 80, 785, 173, 734, 435, 809, 198, 505, 578, 474, 373, 124, 568, 202, 85, 292, 140, 739, 22, 367, 732, 983, 897, 761, 297, 431, 109, 631, 776, 396, 841, 539, 796, 27, 352, 731, 782, 533, 194, 101, 96, 718, 997, 453, 291, 791, 861, 736, 914, 455, 980, 248, 508, 512, 965, 689, 599, 354, 727, 457, 92, 141, 553, 2, 113, 139, 523, 563, 45, 781, 285, 847, 379, 236, 668, 935, 798, 940, 860, 872, 726, 716, 844, 446, 661, 562, 730, 333, 614, 149, 885, 836, 341, 708, 279, 293, 958, 166, 447, 723, 922, 272, 157, 90, 790, 476, 26, 345, 937, 246, 387, 932, 511, 144, 753, 788, 951, 24, 167, 574, 939, 364, 360, 8, 41, 274, 690, 852, 98, 190, 332, 903, 52, 868, 822, 499, 55, 356, 863, 894, 883, 418, 88, 918, 254, 532, 470, 391, 441, 692, 416, 611, 77, 256, 779, 494, 871, 151, 659, 118, 490, 225, 725, 108, 874, 527, 507, 444, 755, 382, 238, 54, 979, 657, 69, 403, 244, 907, 195, 850, 888, 787, 247, 428, 148, 487, 876, 103, 740, 767, 116, 715, 277, 120, 241, 327, 877, 635, 989, 486, 355, 57, 811, 672, 344, 825, 290, 714, 136, 521, 392, 751, 175, 451, 548, 119, 188, 784, 172, 902, 177, 13, 504, 324, 678, 697, 262, 679, 304, 250, 646, 311, 74, 207, 132, 807, 931, 733, 317, 438, 924, 895, 842, 687, 766, 694, 224, 879, 181, 161, 310, 393, 642, 955, 484, 170, 933, 993, 588, 220, 397, 959, 724, 370, 205, 7, 29, 361, 228, 16, 239, 436, 425, 300, 251, 159, 212, 676, 309, 48, 47, 138, 462, 911, 549, 114, 554, 60, 64, 928, 133, 794, 400, 763, 722, 981, 145, 582, 867, 340, 93, 623, 900, 529, 630, 950, 448, 506, 923, 409, 160, 463, 430, 519, 637, 63, 606, 695, 622, 834, 294, 921, 4, 556, 953, 575, 21, 748, 123, 926, 944, 5, 464, 760, 184, 273, 756, 495, 445, 778, 218, 336, 500, 213, 780, 669, 316, 59, 399, 843, 68, 641, 187, 654, 561, 245, 434, 986, 757, 970, 231, 919, 828, 945, 587, 328, 954, 859, 998, 801, 169, 712, 591, 584, 896, 572, 764, 773, 547, 517, 976, 62, 677, 848, 0, 540, 347, 627, 164, 702, 915, 459, 813, 73, 899, 312, 564, 283, 174, 201, 912, 142, 851, 146, 719, 3, 987, 832, 531, 693, 439, 663, 985, 864, 129, 405, 350, 664, 282, 179, 230, 558, 458, 984, 255, 427, 338, 838, 671, 44, 754, 613, 53, 468, 372, 46, 31, 881, 522, 990, 747, 800, 581, 824, 467, 280, 421, 743, 17, 758, 398, 535, 516, 502, 948, 542, 331, 509, 178, 978, 305, 208, 413, 483, 348, 988, 538, 513, 802, 183, 18, 211, 557, 839, 596, 830, 817, 634, 94, 150, 65, 222, 89, 39, 128, 595, 318, 40, 652, 804, 437, 440, 306, 821, 269, 389, 232, 105, 12, 910, 917, 353, 559, 424, 249, 158, 927, 460, 50, 432, 534, 649, 846, 270, 176]\n",
      "\n",
      "Hay 333 Indices de Test:\n",
      "[543, 975, 411, 845, 9, 744, 19, 768, 977, 628, 489, 684, 268, 698, 15, 759, 110, 795, 707, 855, 840, 107, 264, 889, 892, 580, 709, 681, 315, 185, 106, 651, 597, 227, 544, 710, 982, 1, 680, 482, 878, 775, 496, 546, 660, 662, 78, 221, 925, 675, 514, 655, 265, 996, 278, 485, 200, 833, 182, 593, 600, 339, 266, 449, 366, 603, 963, 545, 406, 789, 969, 450, 812, 388, 369, 257, 226, 115, 83, 95, 967, 819, 287, 552, 363, 235, 426, 147, 571, 33, 633, 602, 91, 735, 466, 497, 741, 618, 866, 275, 837, 624, 890, 585, 835, 104, 51, 865, 617, 492, 72, 541, 276, 639, 973, 808, 605, 386, 737, 342, 738, 673, 567, 422, 401, 30, 961, 443, 503, 14, 742, 320, 112, 765, 942, 130, 701, 414, 469, 407, 909, 210, 122, 81, 526, 402, 887, 873, 938, 472, 706, 674, 619, 745, 35, 688, 501, 590, 831, 936, 261, 217, 429, 670, 346, 607, 378, 376, 209, 156, 551, 199, 728, 827, 180, 964, 565, 263, 685, 682, 648, 898, 253, 729, 384, 805, 417, 303, 415, 601, 325, 858, 870, 566, 243, 629, 479, 478, 377, 242, 772, 806, 853, 904, 28, 314, 442, 288, 330, 720, 650, 488, 703, 368, 920, 131, 934, 419, 240, 365, 854, 994, 477, 612, 322, 258, 298, 154, 609, 810, 329, 783, 215, 408, 968, 56, 70, 349, 705, 777, 99, 537, 711, 721, 234, 616, 995, 750, 815, 656, 826, 454, 351, 216, 299, 916, 117, 644, 383, 326, 696, 770, 653, 493, 192, 632, 592, 371, 357, 594, 901, 20, 313, 412, 525, 43, 125, 823, 260, 284, 153, 259, 452, 713, 295, 471, 11, 385, 510, 135, 683, 816, 102, 608, 10, 972, 992, 625, 394, 61, 829, 321, 570, 203, 799, 86, 127, 395, 281, 793, 381, 87, 638, 76, 947, 869, 579, 962, 577, 524, 374, 906, 908, 610, 481, 956, 536, 586, 576, 42, 296, 621, 465]\n",
      "##################################\n",
      "Hay 666 indices de Train:\n",
      "[857, 929, 75, 974, 746, 971, 643, 71, 204, 25, 893, 856, 598, 308, 137, 475, 491, 691, 168, 111, 461, 749, 319, 302, 456, 528, 573, 991, 196, 36, 717, 155, 49, 334, 358, 32, 152, 193, 229, 949, 762, 645, 286, 560, 803, 237, 423, 337, 518, 79, 875, 640, 882, 58, 941, 335, 913, 569, 769, 530, 362, 665, 252, 797, 700, 480, 404, 191, 884, 960, 214, 359, 792, 520, 323, 647, 37, 820, 886, 97, 957, 862, 163, 686, 667, 390, 966, 410, 849, 375, 233, 771, 307, 752, 67, 636, 186, 620, 891, 774, 626, 905, 498, 206, 999, 704, 420, 343, 171, 380, 699, 165, 818, 23, 301, 786, 515, 943, 126, 143, 219, 583, 6, 66, 658, 952, 121, 38, 84, 930, 814, 880, 197, 604, 289, 946, 433, 223, 82, 615, 666, 100, 550, 473, 589, 34, 189, 162, 555, 134, 267, 80, 785, 173, 734, 435, 809, 198, 505, 578, 474, 373, 124, 568, 202, 85, 292, 140, 739, 22, 367, 732, 983, 897, 761, 297, 431, 109, 631, 776, 396, 841, 539, 796, 27, 352, 731, 782, 533, 194, 101, 96, 718, 997, 453, 291, 791, 861, 736, 914, 455, 980, 248, 508, 512, 965, 689, 599, 354, 727, 457, 92, 141, 553, 2, 113, 139, 523, 563, 45, 781, 285, 847, 379, 236, 668, 935, 798, 940, 860, 872, 726, 716, 844, 446, 661, 562, 730, 333, 614, 149, 885, 836, 341, 708, 279, 293, 958, 166, 447, 723, 922, 272, 157, 90, 790, 476, 26, 345, 937, 246, 387, 932, 511, 144, 753, 788, 951, 24, 167, 574, 939, 364, 360, 8, 41, 274, 690, 852, 98, 190, 332, 903, 52, 868, 822, 499, 55, 356, 863, 894, 883, 418, 88, 918, 254, 532, 470, 391, 441, 692, 416, 611, 77, 256, 779, 494, 871, 151, 659, 118, 490, 225, 725, 108, 874, 527, 507, 444, 755, 382, 238, 54, 979, 657, 69, 403, 244, 907, 195, 850, 888, 787, 543, 975, 411, 845, 9, 744, 19, 768, 977, 628, 489, 684, 268, 698, 15, 759, 110, 795, 707, 855, 840, 107, 264, 889, 892, 580, 709, 681, 315, 185, 106, 651, 597, 227, 544, 710, 982, 1, 680, 482, 878, 775, 496, 546, 660, 662, 78, 221, 925, 675, 514, 655, 265, 996, 278, 485, 200, 833, 182, 593, 600, 339, 266, 449, 366, 603, 963, 545, 406, 789, 969, 450, 812, 388, 369, 257, 226, 115, 83, 95, 967, 819, 287, 552, 363, 235, 426, 147, 571, 33, 633, 602, 91, 735, 466, 497, 741, 618, 866, 275, 837, 624, 890, 585, 835, 104, 51, 865, 617, 492, 72, 541, 276, 639, 973, 808, 605, 386, 737, 342, 738, 673, 567, 422, 401, 30, 961, 443, 503, 14, 742, 320, 112, 765, 942, 130, 701, 414, 469, 407, 909, 210, 122, 81, 526, 402, 887, 873, 938, 472, 706, 674, 619, 745, 35, 688, 501, 590, 831, 936, 261, 217, 429, 670, 346, 607, 378, 376, 209, 156, 551, 199, 728, 827, 180, 964, 565, 263, 685, 682, 648, 898, 253, 729, 384, 805, 417, 303, 415, 601, 325, 858, 870, 566, 243, 629, 479, 478, 377, 242, 772, 806, 853, 904, 28, 314, 442, 288, 330, 720, 650, 488, 703, 368, 920, 131, 934, 419, 240, 365, 854, 994, 477, 612, 322, 258, 298, 154, 609, 810, 329, 783, 215, 408, 968, 56, 70, 349, 705, 777, 99, 537, 711, 721, 234, 616, 995, 750, 815, 656, 826, 454, 351, 216, 299, 916, 117, 644, 383, 326, 696, 770, 653, 493, 192, 632, 592, 371, 357, 594, 901, 20, 313, 412, 525, 43, 125, 823, 260, 284, 153, 259, 452, 713, 295, 471, 11, 385, 510, 135, 683, 816, 102, 608, 10, 972, 992, 625, 394, 61, 829, 321, 570, 203, 799, 86, 127, 395, 281, 793, 381, 87, 638, 76, 947, 869, 579, 962, 577, 524, 374, 906, 908, 610, 481, 956, 536, 586, 576, 42, 296, 621, 465]\n",
      "\n",
      "Hay 333 Indices de Test:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247, 428, 148, 487, 876, 103, 740, 767, 116, 715, 277, 120, 241, 327, 877, 635, 989, 486, 355, 57, 811, 672, 344, 825, 290, 714, 136, 521, 392, 751, 175, 451, 548, 119, 188, 784, 172, 902, 177, 13, 504, 324, 678, 697, 262, 679, 304, 250, 646, 311, 74, 207, 132, 807, 931, 733, 317, 438, 924, 895, 842, 687, 766, 694, 224, 879, 181, 161, 310, 393, 642, 955, 484, 170, 933, 993, 588, 220, 397, 959, 724, 370, 205, 7, 29, 361, 228, 16, 239, 436, 425, 300, 251, 159, 212, 676, 309, 48, 47, 138, 462, 911, 549, 114, 554, 60, 64, 928, 133, 794, 400, 763, 722, 981, 145, 582, 867, 340, 93, 623, 900, 529, 630, 950, 448, 506, 923, 409, 160, 463, 430, 519, 637, 63, 606, 695, 622, 834, 294, 921, 4, 556, 953, 575, 21, 748, 123, 926, 944, 5, 464, 760, 184, 273, 756, 495, 445, 778, 218, 336, 500, 213, 780, 669, 316, 59, 399, 843, 68, 641, 187, 654, 561, 245, 434, 986, 757, 970, 231, 919, 828, 945, 587, 328, 954, 859, 998, 801, 169, 712, 591, 584, 896, 572, 764, 773, 547, 517, 976, 62, 677, 848, 0, 540, 347, 627, 164, 702, 915, 459, 813, 73, 899, 312, 564, 283, 174, 201, 912, 142, 851, 146, 719, 3, 987, 832, 531, 693, 439, 663, 985, 864, 129, 405, 350, 664, 282, 179, 230, 558, 458, 984, 255, 427, 338, 838, 671, 44, 754, 613, 53, 468, 372, 46, 31, 881, 522, 990, 747, 800, 581, 824, 467, 280, 421, 743, 17, 758, 398, 535, 516, 502, 948, 542, 331, 509, 178, 978, 305, 208, 413, 483, 348, 988, 538, 513, 802, 183, 18, 211, 557, 839, 596, 830, 817, 634, 94, 150, 65, 222, 89, 39, 128, 595, 318, 40, 652, 804, 437, 440, 306, 821, 269, 389, 232, 105, 12, 910, 917, 353, 559, 424, 249, 158, 927, 460, 50, 432, 534, 649, 846, 270, 176]\n",
      "##################################\n"
     ]
    }
   ],
   "source": [
    "# Se imprime un bucle para que se vean las particiones con sus correspondientes indices de Train y Test, además del número \n",
    "# de indices para que se demuestre la proporción correcta indicada antes.\n",
    "# (Si se quiere reducir la cantidad de texto por pantalla para despues, comente los fors enteros.)\n",
    "# Validacion Simple para dataset de Tic-Tac-Toe.\n",
    "print(\"> Validacion Simple Tic-Tac-Toe\")\n",
    "array_particiones = vs.creaParticiones(datasettictac.datos, None)\n",
    "for particion in array_particiones:\n",
    "    print(\"Hay \" + str(len(particion.indicesTrain)) + \" indices de Train:\")\n",
    "    print(particion.indicesTrain)\n",
    "    print(\"\\nHay \" + str(len(particion.indicesTest)) + \" Indices de Test:\")\n",
    "    print(particion.indicesTest)\n",
    "    print(\"##################################\")\n",
    "\n",
    "# Validacion Simple para dataset de German.\n",
    "print(\"\\n> Validacion Simple German\")\n",
    "array_particiones = vs.creaParticiones(datasetgerman.datos, None)\n",
    "for particion in array_particiones:\n",
    "    print(\"Hay \" + str(len(particion.indicesTrain)) + \" indices de Train:\")\n",
    "    print(particion.indicesTrain)\n",
    "    print(\"\\nHay \" + str(len(particion.indicesTest)) + \" Indices de Test:\")\n",
    "    print(particion.indicesTest)\n",
    "    print(\"##################################\")\n",
    "\n",
    "# Se imprime un bucle para que se vean las K particiones con sus correspondientes indices de Train y Test, además del número \n",
    "# de indices para que se observe el correcto particionado de los datos dentro de las particiones (K-1 folds para Train y 1 \n",
    "# para Test).\n",
    "# Validacion Cruzada para dataset de Tic-Tac-Toe. \n",
    "print(\"\\n> Validacion Cruzada Tic-Tac-Toe\")\n",
    "array_particiones = vc.creaParticiones(datasettictac.datos, None)\n",
    "for particion in array_particiones:\n",
    "    print(\"Hay \" + str(len(particion.indicesTrain)) + \" indices de Train:\")\n",
    "    print(particion.indicesTrain)\n",
    "    print(\"\\nHay \" + str(len(particion.indicesTest)) + \" Indices de Test:\")\n",
    "    print(particion.indicesTest)\n",
    "    print(\"##################################\")\n",
    "\n",
    "# Validacion Cruzada para dataset de German.\n",
    "print(\"\\n> Validacion Cruzada German\")\n",
    "array_particiones = vc.creaParticiones(datasetgerman.datos, None)\n",
    "for particion in array_particiones:\n",
    "    print(\"Hay \" + str(len(particion.indicesTrain)) + \" indices de Train:\")\n",
    "    print(particion.indicesTrain)\n",
    "    print(\"\\nHay \" + str(len(particion.indicesTest)) + \" Indices de Test:\")\n",
    "    print(particion.indicesTest)\n",
    "    print(\"##################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se puede ver cómo las particiones se generan correctamente, respetando el número que se le ha indicado y que la proporción de cada partición es la esperada. En el caso de Validación Simple, es la introducida al crear la estrategia que sería un 50% de los datos para entrenamiento y el otro 50% para validación; y en el caso de Validación Cruzada se cumple K-1 folds, es decir, 2 folds con un 66% total de los datos entre ambos para el entrenamiento, y el fold restante, 1 con un 33%, para validación.  \n",
    "\n",
    "\n",
    "- Además, la generación de los índices es sin sesgo, de manera que todos los datos pertenecientes a un particionado se han generado de manera aleatoria (en este caso sin una semilla, pero se podría haber usado al llamar a creaParticiones()).  \n",
    "\n",
    "\n",
    "- La principal ventaja de Validación Cruzada frente a la Simple es que proporciona un entrenamiento con menor sesgo ya que las particiones acaban teniendo en cuenta todos los datos en Train al ir alternando de fold Test entre los que se ha hecho la división del dataset completo. Sin embargo, una desventaja es esto mismo, ya que puede llevar a un sobreajuste al modelo que se entrene con Validación Cruzada porque tiene en cuenta al final todos los datos en el entrenamiento.  \n",
    "\n",
    "\n",
    "- Por el contrario, la Validación Simple puede proporcionar un sesgo mayor al no tener en cuenta todos los datos en el momento del entrenamiento. Otra desventaja es que esta suele llevar a un porcentaje de error más variable, ya que los datos de Train y Test están aleatoriamente distribuidos en la partición, por lo que las distintas particiones tendrán Tests con más o menos datos ya utilizados en los Tests de otras particiones, no como en Validación Cruzada que en cada partición el fold Test cambia completamente. Una ventaja frente a Validación Cruzada es que es menos costosa computacionalmente, al no tener que hacer las múltiples divisiones y asignaciones de los folds en cada partición.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APARTADO 2\n",
    "En este apartado se realizó el clasificador de Naive Bayes, que supone independientes todos los atributos dada la clase y la clasificación la realiza en función de las probabilidades a posteriori de las clases.\n",
    "\n",
    "- Se van a aprovechar los datasets del apartado anterior para la demostración de este. Vamos a utilizar unas estrategias de particionado nuevas para este apartado.\n",
    "\n",
    "- Lo primero, creamos un clasificador de NB con la variable de Laplace a True y otro con Laplace a False. Esta variable indica si se aplica Laplace a las tablas de atributos en el caso de que exista algún 0 dentro, para evitar probabilidades nulas en los cálculos. Por lo tanto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación Simple con una proporción del 25% para Test y 4 particiones.\n",
    "vs = ValidacionSimple(0.25, 4)\n",
    "# Validación Cruzada con K=4 particiones totales.\n",
    "vc = ValidacionCruzada(4) \n",
    "\n",
    "# Clasificador de NB con Laplace\n",
    "nb_laplace = ClasificadorNaiveBayes(True)\n",
    "# Clasificador de NB sin Laplace\n",
    "nb_nolaplace = ClasificadorNaiveBayes(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una vez tenemos los clasificadores, hay que llamar a la función validacion() de estos. Esta función realiza todo el proceso de entrenamiento y de validación de los resultados obtenidos de las predicciones. En el entrenamiento, se crean las tablas de atributos para las verosimilitudes y una tabla con las prioris de las clases, calculándolas recorriendo los datos correspondientes a los índices de Train de la partición actual. Y, con estos valores obtenidos (lo que es igual a tener el modelo entrenado), se procede a realizar el cálculo de las probabilidades a posteriori de las clases, en función de sus verosimilitudes y sus prioris. Con estas posterioris calculadas, se aplica la decisión MAP (mayor probabilidad a posteriori) de las posterioris de una clase para la clasificación de los datos de Test, obteniendo las predicciones finales del clasificador NB. \n",
    "\n",
    "\n",
    "- Después de esto, se procede a calcular la tasa de error del clasificador con su función error(). Esta devuelve el número de clasificaciones incorrectas de las predicciones de NB sobre los datos de Test, comparándolas con los valores reales de sus clases. Por lo tanto, se devuelve al final un array con las tasas de error correspondientes a cada una de las particiones.\n",
    "\n",
    "\n",
    "- Se ha realizado todo esto por cada uno de los datasets, utilizando cada uno de los clasificadores y cada una de las estrategias de particionado. Y se ha calculado la media y la desviación típica de las tasas de errores de cada caso, para hacer una tabla después. Además, se guardarán las matrices de confusión y las tasas de confusión de cada uno de los clasificadores para cada uno de los casos. Entonces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario con las medias y desviaciones para la posterior tabla.\n",
    "dicc_med_desv = {}\n",
    "dicc_med_desv['Laplace'] = {}\n",
    "dicc_med_desv['NoLaplace'] = {}\n",
    "dicc_med_desv['Laplace']['VS'] = {}\n",
    "dicc_med_desv['NoLaplace']['VS'] = {}\n",
    "dicc_med_desv['Laplace']['VC'] = {}\n",
    "dicc_med_desv['NoLaplace']['VC'] = {}\n",
    "\n",
    "# Diccionario con las matrices de confusión\n",
    "dicc_matrices = {}\n",
    "dicc_matrices['Laplace'] = {}\n",
    "dicc_matrices['NoLaplace'] = {}\n",
    "dicc_matrices['Laplace']['VS'] = {}\n",
    "dicc_matrices['NoLaplace']['VS'] = {}\n",
    "dicc_matrices['Laplace']['VC'] = {}\n",
    "dicc_matrices['NoLaplace']['VC'] = {}\n",
    "\n",
    "# Diccionario con las tasas de confusión\n",
    "dicc_tasas = {}\n",
    "dicc_tasas['Laplace'] = {}\n",
    "dicc_tasas['NoLaplace'] = {}\n",
    "dicc_tasas['Laplace']['VS'] = {}\n",
    "dicc_tasas['NoLaplace']['VS'] = {}\n",
    "dicc_tasas['Laplace']['VC'] = {}\n",
    "dicc_tasas['NoLaplace']['VC'] = {}\n",
    "\n",
    "# Función auxiliar para los cálculos de la tabla.\n",
    "def calcula_med_desv(tasas_error, rev):\n",
    "    n = len(tasas_error)\n",
    "    media = 0\n",
    "    desviacion = 0\n",
    "    for error in tasas_error:\n",
    "        if rev:\n",
    "            error = 1 - error\n",
    "        media += error\n",
    "    media /= n\n",
    "    \n",
    "    for error in tasas_error:\n",
    "        if rev:\n",
    "            error = 1 - error\n",
    "        desviacion += (error - media)**2\n",
    "    desviacion = math.sqrt(desviacion/n)\n",
    "    \n",
    "    return media, desviacion;\n",
    "\n",
    "\n",
    "# NB con Laplace y Validación Simple sobre Tic-Tac-Toe.\n",
    "tasas_error = nb_laplace.validacion(vs, datasettictac, nb_laplace, None)\n",
    "media,desviacion = calcula_med_desv(tasas_error, False)\n",
    "dicc_med_desv['Laplace']['VS']['TTT_MED'] = media\n",
    "dicc_med_desv['Laplace']['VS']['TTT_DESV'] = desviacion\n",
    "dicc_matrices['Laplace']['VS']['TTT'] = nb_laplace.matriz_confusion\n",
    "dicc_tasas['Laplace']['VS']['TTT'] = nb_laplace.tasas_confusion\n",
    "\n",
    "# NB con Laplace y Validación Simple sobre German.\n",
    "tasas_error = nb_laplace.validacion(vs, datasetgerman, nb_laplace, None)\n",
    "media,desviacion = calcula_med_desv(tasas_error, False)\n",
    "dicc_med_desv['Laplace']['VS']['GER_MED'] = media\n",
    "dicc_med_desv['Laplace']['VS']['GER_DESV'] = desviacion\n",
    "dicc_matrices['Laplace']['VS']['GER'] = nb_laplace.matriz_confusion\n",
    "dicc_tasas['Laplace']['VS']['GER'] = nb_laplace.tasas_confusion\n",
    "\n",
    "# NB con Laplace y Validación Cruzada sobre Tic-Tac-Toe.\n",
    "tasas_error = nb_laplace.validacion(vc, datasettictac, nb_laplace, None)\n",
    "media,desviacion = calcula_med_desv(tasas_error, False)\n",
    "dicc_med_desv['Laplace']['VC']['TTT_MED'] = media\n",
    "dicc_med_desv['Laplace']['VC']['TTT_DESV'] = desviacion\n",
    "dicc_matrices['Laplace']['VC']['TTT'] = nb_laplace.matriz_confusion\n",
    "dicc_tasas['Laplace']['VC']['TTT'] = nb_laplace.tasas_confusion\n",
    "\n",
    "# NB con Laplace y Validación Cruzada sobre German.\n",
    "tasas_error = nb_laplace.validacion(vc, datasetgerman, nb_laplace, None)\n",
    "media,desviacion = calcula_med_desv(tasas_error, False)\n",
    "dicc_med_desv['Laplace']['VC']['GER_MED'] = media\n",
    "dicc_med_desv['Laplace']['VC']['GER_DESV'] = desviacion\n",
    "dicc_matrices['Laplace']['VC']['GER'] = nb_laplace.matriz_confusion\n",
    "dicc_tasas['Laplace']['VC']['GER'] = nb_laplace.tasas_confusion\n",
    "\n",
    "# NB sin Laplace y Validación Simple sobre Tic-Tac-Toe.\n",
    "tasas_error = nb_nolaplace.validacion(vs, datasettictac, nb_nolaplace, None)\n",
    "media,desviacion = calcula_med_desv(tasas_error, False)\n",
    "dicc_med_desv['NoLaplace']['VS']['TTT_MED'] = media\n",
    "dicc_med_desv['NoLaplace']['VS']['TTT_DESV'] = desviacion\n",
    "dicc_matrices['NoLaplace']['VS']['TTT'] = nb_nolaplace.matriz_confusion\n",
    "dicc_tasas['NoLaplace']['VS']['TTT'] = nb_nolaplace.tasas_confusion\n",
    "\n",
    "# NB sin Laplace y Validación Simple sobre German.\n",
    "tasas_error = nb_nolaplace.validacion(vs, datasetgerman, nb_nolaplace, None)\n",
    "media,desviacion = calcula_med_desv(tasas_error, False)\n",
    "dicc_med_desv['NoLaplace']['VS']['GER_MED'] = media\n",
    "dicc_med_desv['NoLaplace']['VS']['GER_DESV'] = desviacion\n",
    "dicc_matrices['NoLaplace']['VS']['GER'] = nb_nolaplace.matriz_confusion\n",
    "dicc_tasas['NoLaplace']['VS']['GER'] = nb_nolaplace.tasas_confusion\n",
    "\n",
    "# NB sin Laplace y Validación Cruzada sobre Tic-Tac-Toe.\n",
    "tasas_error = nb_nolaplace.validacion(vc, datasettictac, nb_nolaplace, None)\n",
    "media,desviacion = calcula_med_desv(tasas_error, False)\n",
    "dicc_med_desv['NoLaplace']['VC']['TTT_MED'] = media\n",
    "dicc_med_desv['NoLaplace']['VC']['TTT_DESV'] = desviacion\n",
    "dicc_matrices['NoLaplace']['VC']['TTT'] = nb_nolaplace.matriz_confusion\n",
    "dicc_tasas['NoLaplace']['VC']['TTT'] = nb_nolaplace.tasas_confusion\n",
    "\n",
    "# NB sin Laplace y Validación Cruzada sobre German.\n",
    "tasas_error = nb_nolaplace.validacion(vc, datasetgerman, nb_nolaplace, None)\n",
    "media,desviacion = calcula_med_desv(tasas_error, False)\n",
    "dicc_med_desv['NoLaplace']['VC']['GER_MED'] = media\n",
    "dicc_med_desv['NoLaplace']['VC']['GER_DESV'] = desviacion\n",
    "dicc_matrices['NoLaplace']['VC']['GER'] = nb_nolaplace.matriz_confusion\n",
    "dicc_tasas['NoLaplace']['VC']['GER'] = nb_nolaplace.tasas_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A continuación, la tabla con las medias y desviaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Tabla con medias y desviaciones del clasificador NB con Laplace:\n",
      "\n",
      "    |  TTT_μ   |   TTT_σ  |   GER_μ   |   GER_σ  |\n",
      " VS | 0.301255 | 0.031311 | 0.251000  | 0.017748 |\n",
      " VC | 0.294979 | 0.028761 | 0.268000  | 0.021541 |\n",
      "\n",
      "\n",
      "> Tabla con medias y desviaciones del clasificador NB sin Laplace:\n",
      "\n",
      "    |  TTT_μ   |   TTT_σ  |   GER_μ   |   GER_σ  |\n",
      " VS | 0.300209 | 0.030658 | 0.241000  | 0.009110 |\n",
      " VC | 0.299163 | 0.043129 | 0.254000  | 0.015362 |\n"
     ]
    }
   ],
   "source": [
    "#print(dicc_med_desv)\n",
    "# *Forma tabla*\n",
    "# Usando Laplace:   | TTT_μ | TTT_σ | GER_μ | GER_σ |\n",
    "#                 VS|       |       |       |       |\n",
    "#                 VC|       |       |       |       |\n",
    "# VS == Se usa estrategia Validación Simple\n",
    "# VC == Se usa estrategia Validación Cruzada\n",
    "# TXT_μ == Media μ de las tasas de error sobre el texto TXT\n",
    "# TXT_σ == Desviación típica σ de las tasas de error sobre el texto TXT\n",
    "# TTT == TXT de Tic-Tac-Toe.data\n",
    "# GER == TXT de German.data\n",
    "\n",
    "print(\"> Tabla con medias y desviaciones del clasificador NB con Laplace:\\n\")\n",
    "print('    {4}  {0:2}   {4}   {1:3}  {4}   {2:4}   {4}   {3:5}  {4}'.format(\"TTT_μ\", \"TTT_σ\", \"GER_μ\", \"GER_σ\",'|'))\n",
    "print(' VS {4} {0:2f} {4} {1:3f} {4} {2:4f}  {4} {3:5f} {4}'.format(dicc_med_desv['Laplace']['VS']['TTT_MED'] , dicc_med_desv['Laplace']['VS']['TTT_DESV'], dicc_med_desv['Laplace']['VS']['GER_MED'], dicc_med_desv['Laplace']['VS']['GER_DESV'],'|'))\n",
    "print(' VC {4} {0:2f} {4} {1:3f} {4} {2:4f}  {4} {3:5f} {4}'.format(dicc_med_desv['Laplace']['VC']['TTT_MED'] , dicc_med_desv['Laplace']['VC']['TTT_DESV'], dicc_med_desv['Laplace']['VC']['GER_MED'], dicc_med_desv['Laplace']['VC']['GER_DESV'],'|'))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"> Tabla con medias y desviaciones del clasificador NB sin Laplace:\\n\")\n",
    "print('    {4}  {0:2}   {4}   {1:3}  {4}   {2:4}   {4}   {3:5}  {4}'.format(\"TTT_μ\", \"TTT_σ\", \"GER_μ\", \"GER_σ\",'|'))\n",
    "print(' VS {4} {0:2f} {4} {1:3f} {4} {2:4f}  {4} {3:5f} {4}'.format(dicc_med_desv['NoLaplace']['VS']['TTT_MED'] , dicc_med_desv['NoLaplace']['VS']['TTT_DESV'], dicc_med_desv['NoLaplace']['VS']['GER_MED'], dicc_med_desv['NoLaplace']['VS']['GER_DESV'],'|'))\n",
    "print(' VC {4} {0:2f} {4} {1:3f} {4} {2:4f}  {4} {3:5f} {4}'.format(dicc_med_desv['NoLaplace']['VC']['TTT_MED'] , dicc_med_desv['NoLaplace']['VC']['TTT_DESV'], dicc_med_desv['NoLaplace']['VC']['GER_MED'], dicc_med_desv['NoLaplace']['VC']['GER_DESV'],'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se puede observar cómo la media μ de las tasas de errores del clasificador con Laplace vale alrededor del 27% y la media del clasificador sin Laplace vale alrededor del 28% también. Además, la desviación típica σ de las tasas de errores respecto a sus medias μ no es nada grande, tan solo de un 2% aprox. en NB con Laplace y sin Laplace.\n",
    "\n",
    "\n",
    "- Con los atributos elegidos para que las proporciones fueran más o menos iguales para Validación Cruzada y Validación Simple (1/4 de proporción para el Test) se puede ver que las medias de las tasas de errores usando ambas estrategias es muy parecida. Sin embargo, se puede notar, no por mucho, que la Validación Cruzada parece haber tenido mejores resultados en este apartado que la Validación Simple. \n",
    "\n",
    "\n",
    "- En cuanto a las ventajas/desventajas de VS y VC dichas en el anterior apartado, nos hemos dado cuenta de que las medias de las tasas de error de Validación Simple son, efectivamente, más variables que las de Validación Cruzada. Esto se puede ver reflejado en la desviación porque es un poco mayor cuando se usa Validación Simple. Y, en cuanto a las medias, Validación Cruzada es en la teoría la que menor tasa de error general debería producir y se puede confirmar con las múltiples pruebas que hemos hecho sobre estos datasets, donde ha tenido menores medias que la Validación Simple.\n",
    "\n",
    "\n",
    "- En cuanto a la corrección de Laplace, esta se ha visto, por lo dicho en el primer párrafo, que es un poco menor la media del error cuando se aplica en el clasificador de NB. Esto puede ser debido a que las probabilidades nulas sí tienen una representación importante al corregirse y no valer 0, lo que acaba mejorando la probabilidad a posteriori.\n",
    "\n",
    "\n",
    "- Esto quiere decir que las clasificaciones de nuestro modelo de Naive Bayes poseen un acierto del 70-75% y un error del 25-30% de manera general. Es un buen porcentaje de error pero que se podría mejorar con otro modelo seguramente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APARTADO 3\n",
    "En este apartado se ha vuelto a realizar lo mismo que en el apartado anterior pero se han utilizado las funciones pertenecientes al paquete de scikit-learn.\n",
    "\n",
    "- Primero leemos los datos de Tic-Tac-Toe.data de nuevo y reutilizamos el dataset de German.data de los apartados anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ttt_scikit = pd.read_csv(\"./tic-tac-toe.data\")\n",
    "'''P*\n",
    "#Caso probado para dataset nuestro de Tic-Tac-Toe.\n",
    "data_ttt_scikit = datasettictac.datos'''\n",
    "data_ger_scikit = datasetgerman.datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dividimos los datasets en dos conjuntos: X, que posee las columnas de los atributos que no son la clase; y, que posee la columna de la clase. Hacemos esto porque es necesario para luego aplicar los métodos de scikit-learn. Por lo tanto, hacemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X e y del dataset de Tic-Tac-Toe\n",
    "nombres_atrs = list(data_ttt_scikit.columns)\n",
    "nombres_atrs.remove('Class')\n",
    "X_train_ttt = data_ttt_scikit[nombres_atrs].values\n",
    "y_train_ttt = data_ttt_scikit['Class'].values\n",
    "'''P* \n",
    "#Caso probado para dataset nuestro de Tic-Tac-Toe.\n",
    "X_train_ttt = []\n",
    "y_train_ttt = []\n",
    "for dato in data_ttt_scikit:\n",
    "    y_train_ttt.append(dato[-1])\n",
    "    X_train_ttt.append(dato[0:len(dato)-1])'''\n",
    "\n",
    "# X e y del dataset de German\n",
    "X_train_ger = []\n",
    "y_train_ger = []\n",
    "for dato in data_ger_scikit:\n",
    "    y_train_ger.append(dato[-1])\n",
    "    X_train_ger.append(dato[0:len(dato)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora, procedemos a crear los clasificadores que vamos a utilizar. Estos serán los mismos que los creados en el apartado 2. Sin embargo, no podemos aplicar Laplace en el modelo GaussianNB, por lo que no haremos esta comparación entre usar Laplace o no con el dataset de German. Como utilizamos el modelo MultinomialNB para Tic-Tac-Toe, para este sí haremos la comparación al poderse realizar esta corrección. Por lo tanto, los creamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo NB con la corrección de Laplace (para Tic-Tac-Toe)\n",
    "cf_multi_laplace = MultinomialNB(alpha=1)\n",
    "# Modelo NB sin la corrección de Laplace (para Tic-Tac-Toe)\n",
    "cf_multi_nolaplace = MultinomialNB(alpha=0.0000001)\n",
    "# Modelo NB Gaussiano (para German)\n",
    "cf_gaussian = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pero antes de realizar los procesos de entrenamiento y validación de los modelos anteriores, tenemos que preprocesar los datos del conjunto X de Tic-Tac-Toe para que funcionen correctamente los métodos (ya que este conjunto posee variables con valores nominales). En el caso de German, como utilizamos el dataset ya transformado por nosotros, no hace falta aplicar este preprocesado. Entonces, sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''P*\n",
    "#Comentado lo siguiente.'''\n",
    "# Generamos la clase del preprocesamiento\n",
    "enc = OneHotEncoder()\n",
    "# Preprocesamos el conjunto X de Tic-Tac-Toe\n",
    "X_train_ttt = enc.fit_transform(X_train_ttt).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A continuación, realizamos los entrenamientos y las validaciones de los 3 modelos anteriores con la función cross_val_score() del paquete scikit-learn. Esta función genera las particiones mediante Validación Cruzada y posteriormente realiza el proceso de entrenamiento y la validación de sus clasificaciones. Al final, devuelve un array con los \"scores\", que son las precisiones del modelo en cada partición (% de aciertos). Como hemos hecho en el apartado anterior, generamos 4 particiones para que la proporción de cada fold sea del 25%. \n",
    "- Además, se han calculado las medias y desviaciones típicas de cada modelo en cada caso para generar otra tabla y comparar los resultados con la del apartado 2. Por lo tanto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB Multinomial con Laplace y Validación Cruzada sobre Tic-Tac-Toe.\n",
    "scores = cross_val_score(cf_multi_laplace, X_train_ttt, y_train_ttt, cv=4)\n",
    "media,desviacion = calcula_med_desv(scores, True)\n",
    "dicc_med_desv['Laplace']['VC']['TTT_MED'] = media\n",
    "dicc_med_desv['Laplace']['VC']['TTT_DESV'] = desviacion\n",
    "\n",
    "# NB Multinomial sin Laplace y Validación Cruzada sobre Tic-Tac-Toe.\n",
    "scores = cross_val_score(cf_multi_nolaplace, X_train_ttt, y_train_ttt, cv=4)\n",
    "media,desviacion = calcula_med_desv(scores, True)\n",
    "dicc_med_desv['NoLaplace']['VC']['TTT_MED'] = media\n",
    "dicc_med_desv['NoLaplace']['VC']['TTT_DESV'] = desviacion\n",
    "\n",
    "# NB Gaussiano y Validación Cruzada sobre German.\n",
    "scores = cross_val_score(cf_gaussian, X_train_ger, y_train_ger, cv=4)\n",
    "media,desviacion = calcula_med_desv(scores, True)\n",
    "dicc_med_desv['NoLaplace']['VC']['GER_MED'] = media\n",
    "dicc_med_desv['NoLaplace']['VC']['GER_DESV'] = desviacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tras usar la Validación Cruzada, aplicamos la Validación Simple para generar las particiones que se usarán en el entrenamiento y la validación de los modelos anteriores. Esta estrategia de particionado se genera mediante el método train_test_split() de scikit-learn. Creamos un bucle de 4 repeticiones para obtener 4 particiones con una proporción Test del 25%. Tras obtener los nuevos conjuntos X e y correspondientes a las particiones, los utilizamos para entrenar los modelos mediante el método fit() y realizar la validación mediante el método score(). Esta última devuelve también un array de las precisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = []\n",
    "scores2 = []\n",
    "scores3 = []\n",
    "i = 0\n",
    "while i < 4:\n",
    "    X_train_tttaux, X_test_tttaux, y_train_tttaux, y_test_tttaux = train_test_split(X_train_ttt, y_train_ttt, test_size=0.25)\n",
    "    X_train_geraux, X_test_geraux, y_train_geraux, y_test_geraux = train_test_split(X_train_ger, y_train_ger, test_size=0.25)\n",
    "    \n",
    "    #Entrenamos los modelos con los datos de entrenamiento de la particion\n",
    "    cf_multi_laplace.fit(X_train_tttaux, y_train_tttaux)\n",
    "    cf_multi_nolaplace.fit(X_train_tttaux, y_train_tttaux)\n",
    "    cf_gaussian.fit(X_train_geraux, y_train_geraux)\n",
    "\n",
    "    #Calculamos los scores validando con el Test de la particion\n",
    "    # NB Multinomial con Laplace y Validación Simple sobre Tic-Tac-Toe.\n",
    "    scores = cf_multi_laplace.score(X_test_tttaux, y_test_tttaux)\n",
    "    scores1.append(scores)\n",
    "    \n",
    "    # NB Multinomial sin Laplace y Validación Simple sobre Tic-Tac-Toe.\n",
    "    scores = cf_multi_nolaplace.score(X_test_tttaux, y_test_tttaux)\n",
    "    scores2.append(scores)\n",
    "    \n",
    "    # NB Gaussiana y Validación Simple sobre German.\n",
    "    scores = cf_gaussian.score(X_test_geraux, y_test_geraux)\n",
    "    scores3.append(scores)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Calculamos medias y desviaciones de las tasas de error\n",
    "media,desviacion = calcula_med_desv(scores1, True)\n",
    "dicc_med_desv['Laplace']['VS']['TTT_MED'] = media\n",
    "dicc_med_desv['Laplace']['VS']['TTT_DESV'] = desviacion\n",
    "\n",
    "media,desviacion = calcula_med_desv(scores2, True)\n",
    "dicc_med_desv['NoLaplace']['VS']['TTT_MED'] = media\n",
    "dicc_med_desv['NoLaplace']['VS']['TTT_DESV'] = desviacion\n",
    "\n",
    "media,desviacion = calcula_med_desv(scores3, True)\n",
    "dicc_med_desv['NoLaplace']['VS']['GER_MED'] = media\n",
    "dicc_med_desv['NoLaplace']['VS']['GER_DESV'] = desviacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entonces, una vez tenemos todas las medias y desviaciones, hemos creado unas tablas parecidas a las del apartado anterior para que podamos comentar y analizar los resultados obtenidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Tabla con medias y desviaciones del clasificador MultinomialNB sobre Tic-Tac-Toe:\n",
      "\n",
      " MultinomialNB - TTT |  μ_Laplace   |   σ_Laplace  |   μ_NoLaplace   |   σ_NoLaplace  |\n",
      "                 VS  |   0.311458   |   0.022317   |    0.311458     |    0.022317    |\n",
      "                 VC  |   0.407170   |   0.040839   |    0.407170     |    0.040839    |\n",
      "\n",
      "\n",
      "> Tabla con medias y desviaciones del clasificador GaussianNB sobre German:\n",
      "\n",
      " GaussianNB - GER |  μ_NoLaplace   |   σ_NoLaplace  |\n",
      "               VS |    0.261000    |    0.020273    |\n",
      "               VC |    0.266000    |    0.024249    |\n"
     ]
    }
   ],
   "source": [
    "# *Forma tabla*\n",
    "# MultinomialNB - TTT:    | μ_Laplace | σ_Laplace | μ_NoLaplace | σ_NoLaplace |\n",
    "#                       VS|           |           |             |             |\n",
    "#                       VC|           |           |             |             |\n",
    "# VS == Se usa estrategia Validación Simple\n",
    "# VC == Se usa estrategia Validación Cruzada\n",
    "# μ_Laplace == Media μ de las tasas de error aplicando Laplace\n",
    "# σ_Laplace == Desviación típica σ de las tasas de error sin aplicar Laplace\n",
    "\n",
    "print(\"> Tabla con medias y desviaciones del clasificador MultinomialNB sobre Tic-Tac-Toe:\\n\")\n",
    "print(' MultinomialNB - TTT {4}  {0:2}   {4}   {1:3}  {4}   {2:4}   {4}   {3:5}  {4}'.format(\"μ_Laplace\", \"σ_Laplace\", \"μ_NoLaplace\", \"σ_NoLaplace\",'|'))\n",
    "print('                 VS  {4}   {0:2f}   {4}   {1:3f}   {4}    {2:4f}     {4}    {3:5f}    {4}'.format(dicc_med_desv['Laplace']['VS']['TTT_MED'] , dicc_med_desv['Laplace']['VS']['TTT_DESV'], dicc_med_desv['NoLaplace']['VS']['TTT_MED'], dicc_med_desv['NoLaplace']['VS']['TTT_DESV'],'|'))\n",
    "print('                 VC  {4}   {0:2f}   {4}   {1:3f}   {4}    {2:4f}     {4}    {3:5f}    {4}'.format(dicc_med_desv['Laplace']['VC']['TTT_MED'] , dicc_med_desv['Laplace']['VC']['TTT_DESV'], dicc_med_desv['NoLaplace']['VC']['TTT_MED'], dicc_med_desv['NoLaplace']['VC']['TTT_DESV'],'|'))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"> Tabla con medias y desviaciones del clasificador GaussianNB sobre German:\\n\")\n",
    "print(' GaussianNB - GER {2}  {0:2}   {2}   {1:3}  {2}'.format(\"μ_NoLaplace\", \"σ_NoLaplace\",'|'))\n",
    "print('               VS {2}    {0:2f}    {2}    {1:3f}    {2}'.format(dicc_med_desv['NoLaplace']['VS']['GER_MED'] , dicc_med_desv['NoLaplace']['VS']['GER_DESV'], '|'))\n",
    "print('               VC {2}    {0:2f}    {2}    {1:3f}    {2}'.format(dicc_med_desv['NoLaplace']['VC']['GER_MED'] , dicc_med_desv['NoLaplace']['VC']['GER_DESV'], '|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por lo general, se puede observar que las medias de las tasas de error y las desviaciones, de los modelos de las funciones del paquete scikit-learn, son practicamente las mismas que las obtenidas con nuestros clasificadores de Naive Bayes. Esta media vale en torno al 30% cuando se usa Validación Simple y 40% cuando se usa Validación Cruzada, por lo que nuestros clasificadores se realizaron correctamente al poseer resultados parecidos.\n",
    "\n",
    "\n",
    "- En cuanto a las diferencias entre MultinomialNB y GaussianNB, hemos notado que las medias y variaciones de GaussianNB son menores que las obtenidas mediante MultinomialNB con Laplace y sin Laplace.\n",
    "\n",
    "\n",
    "- Hemos observado que los métodos de este paquete generan una media de tasas de errores y una desviación típica un poco mayor que nuestros clasificadores cuando se aplica la estrategia de particionado de Validación Cruzada. Además, estos valores son también mayores que respecto a utilizar la estrategia de Validación Simple, y habíamos indicado anteriormente que lo normal es que Validación Simple suele dar mayores tasas de error debido a su variabilidad, que se reflejaba en la desviación. Hemos realizado varias pruebas y los valores parecen ser, más veces, mayores en Validación Cruzada. Puede ser una suposición, pero puede ser que el preprocesamiento del dataset de Tic-Tac-Toe esté afectando negativamente en este ejemplo en concreto.\n",
    "\n",
    "\n",
    "- Esta suposición parece ser correcta, ya que hemos ejecutado los entrenamientos y las validaciones de los métodos de scikit-learn utilizando directamente nuestro propio dataset de Tic-Tac-Toe generado en el apartado 1 (como habíamos hecho con el de German) para MultinomialNB. Al obtener las tablas anteriores, pero con esta pequeña prueba puntual, hemos visto que las medias de MultinomialNB con y sin Laplace, utilizando Validación Cruzada, han bajado un 5% hasta el 35% de tasa media de error y las desviaciones han cumplido la teoría, es decir, han salido menores las desviaciones cuando se utilizaba Validación Cruzada frente a la Validación Simple. Así que, por esta prueba (repetida varias veces), creemos que el preprocesado de Tic-Tac-Toe ha empeorado un poco al modelo de MultinomialNB con Validación Cruzada. Igualmente, dejamos el preprocesado hecho, ya que es lo que se indica en el enunciado de la práctica. Hemos dejado comentado lo que habíamos cambiado para esta pequeña prueba con P*.\n",
    "\n",
    "\n",
    "- GaussianNB, por el contrario, como utiliza el dataset nuestro de German tiene un rendimiento generalmente bueno y muy parecido al clasificador NB que hemos creado nosotros en el apartado 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APARTADO 4\n",
    "En este último apartado se nos pidió realizar un análisis ROC sobre los clasificadores empleados.\n",
    "\n",
    "- Primero, mostramos las matrices de confusión calculadas en el apartado 2 para cada caso de los dos clasificadores de ese mismo apartado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Matriz de confusion para NB con Laplace y Validación Cruzada sobre Tic-Tac-Toe:\n",
      "\n",
      " |  TP   |  FP  |  FN  |  TN  |\n",
      " |  124  |  51  |  29  |  35  |\n",
      "\n",
      "\n",
      "> Matriz de confusion para NB con Laplace y Validación Simple sobre Tic-Tac-Toe:\n",
      "\n",
      " |  TP   |  FP  |  FN  |  TN  |\n",
      " |  128  |  46  |  26  |  39  |\n",
      "\n",
      "\n",
      "> Matriz de confusion para NB con Laplace y Validación Cruzada sobre German:\n",
      "\n",
      " |  TP   |  FP  |  FN  |  TN   |\n",
      " |  37   |  29  |  47  |  137  |\n",
      "\n",
      "\n",
      "> Matriz de confusion para NB con Laplace y Validación Simple sobre German:\n",
      "\n",
      " |  TP   |  FP  |  FN  |  TN   |\n",
      " |  35   |  30  |  31  |  154  |\n",
      "\n",
      "\n",
      "> Matriz de confusion para NB sin Laplace y Validación Cruzada sobre Tic-Tac-Toe:\n",
      "\n",
      " |  TP   |  FP  |  FN  |  TN  |\n",
      " |  139  |  56  |  14  |  30  |\n",
      "\n",
      "\n",
      "> Matriz de confusion para NB sin Laplace y Validación Simple sobre Tic-Tac-Toe:\n",
      "\n",
      " |  TP   |  FP  |  FN  |  TN  |\n",
      " |  141  |  52  |  14  |  32  |\n",
      "\n",
      "\n",
      "> Matriz de confusion para NB sin Laplace y Validación Cruzada sobre German:\n",
      "\n",
      " |  TP   |  FP  |  FN  |  TN   |\n",
      " |  37   |  22  |  39  |  152  |\n",
      "\n",
      "\n",
      "> Matriz de confusion para NB sin Laplace y Validación Simple sobre German:\n",
      "\n",
      " |  TP   |  FP  |  FN  |  TN   |\n",
      " |  30   |  20  |  42  |  158  |\n"
     ]
    }
   ],
   "source": [
    "#print(dicc_matrices)\n",
    "\n",
    "print(\"> Matriz de confusion para NB con Laplace y Validación Cruzada sobre Tic-Tac-Toe:\\n\")\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(\"TP\", \"FP\", \"FN\", \"TN\",'|'))\n",
    "print(' {4}  {0:2}  {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(dicc_matrices['Laplace']['VC']['TTT'][0], dicc_matrices['Laplace']['VC']['TTT'][1], dicc_matrices['Laplace']['VC']['TTT'][2], dicc_matrices['Laplace']['VC']['TTT'][3],'|'))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"> Matriz de confusion para NB con Laplace y Validación Simple sobre Tic-Tac-Toe:\\n\")\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(\"TP\", \"FP\", \"FN\", \"TN\",'|'))\n",
    "print(' {4}  {0:2}  {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(dicc_matrices['Laplace']['VS']['TTT'][0], dicc_matrices['Laplace']['VS']['TTT'][1], dicc_matrices['Laplace']['VS']['TTT'][2], dicc_matrices['Laplace']['VS']['TTT'][3],'|'))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"> Matriz de confusion para NB con Laplace y Validación Cruzada sobre German:\\n\")\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}   {4}'.format(\"TP\", \"FP\", \"FN\", \"TN\",'|'))\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(dicc_matrices['Laplace']['VC']['GER'][0], dicc_matrices['Laplace']['VC']['GER'][1], dicc_matrices['Laplace']['VC']['GER'][2], dicc_matrices['Laplace']['VC']['GER'][3],'|'))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"> Matriz de confusion para NB con Laplace y Validación Simple sobre German:\\n\")\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}   {4}'.format(\"TP\", \"FP\", \"FN\", \"TN\",'|'))\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(dicc_matrices['Laplace']['VS']['GER'][0], dicc_matrices['Laplace']['VS']['GER'][1], dicc_matrices['Laplace']['VS']['GER'][2], dicc_matrices['Laplace']['VS']['GER'][3],'|'))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"> Matriz de confusion para NB sin Laplace y Validación Cruzada sobre Tic-Tac-Toe:\\n\")\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(\"TP\", \"FP\", \"FN\", \"TN\",'|'))\n",
    "print(' {4}  {0:2}  {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(dicc_matrices['NoLaplace']['VC']['TTT'][0], dicc_matrices['NoLaplace']['VC']['TTT'][1], dicc_matrices['NoLaplace']['VC']['TTT'][2], dicc_matrices['NoLaplace']['VC']['TTT'][3],'|'))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"> Matriz de confusion para NB sin Laplace y Validación Simple sobre Tic-Tac-Toe:\\n\")\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(\"TP\", \"FP\", \"FN\", \"TN\",'|'))\n",
    "print(' {4}  {0:2}  {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(dicc_matrices['NoLaplace']['VS']['TTT'][0], dicc_matrices['NoLaplace']['VS']['TTT'][1], dicc_matrices['NoLaplace']['VS']['TTT'][2], dicc_matrices['NoLaplace']['VS']['TTT'][3],'|'))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"> Matriz de confusion para NB sin Laplace y Validación Cruzada sobre German:\\n\")\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}   {4}'.format(\"TP\", \"FP\", \"FN\", \"TN\",'|'))\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(dicc_matrices['NoLaplace']['VC']['GER'][0], dicc_matrices['NoLaplace']['VC']['GER'][1], dicc_matrices['NoLaplace']['VC']['GER'][2], dicc_matrices['NoLaplace']['VC']['GER'][3],'|'))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"> Matriz de confusion para NB sin Laplace y Validación Simple sobre German:\\n\")\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}   {4}'.format(\"TP\", \"FP\", \"FN\", \"TN\",'|'))\n",
    "print(' {4}  {0:2}   {4}  {1:2}  {4}  {2:2}  {4}  {3:2}  {4}'.format(dicc_matrices['NoLaplace']['VS']['GER'][0], dicc_matrices['NoLaplace']['VS']['GER'][1], dicc_matrices['NoLaplace']['VS']['GER'][2], dicc_matrices['NoLaplace']['VS']['GER'][3],'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se pueden ver las 8 matrices de confusión obtenidas de los resultados de todos los casos usando el clasificador de NB con Laplace y sin Laplace. Estas poseen: TP, verdaderos positivos; FP, falsos positivos; FN, falsos negativos; y TN, verdaderos negativos. Estos valores indican si las clasificaciones de la clase positiva y la clase negativa han acertado o han hecho una clasificación errónea.\n",
    "\n",
    "\n",
    "- Para los conjuntos de datos de Tic-Tac-Toe y German hemos observado que generalmente el Clasificador de Naive Bayes genera un mayor número de verdaderos positivos y verdaderos negativos frente al número de falsos, tanto usando Validación Cruzada como Simple. Estas clasificaciones son correctas ya que estas son variadas y los falsos son, en general, menores.\n",
    "\n",
    "\n",
    "- Queremos señalar que, generalmente, el coste de clasificar como negativo a un dato positivo (FN) es mucho mayor (ej. un paciente con cáncer que le dices que no lo tiene), por lo que se busca evitar que este número sea tan grande. Esto se podría evitar aumentando las clasificaciones como clase positiva al poseer un menor coste al equivocarse (FP).\n",
    "\n",
    "\n",
    "- Estos valores se calculan dentro del método error de la clase Clasificador y se guardan en dos atributos nuevos que hemos creado en la clase ClasificadorNB, uno para la matriz de confusión y otro para las tasas de confusión que veremos a continuación. Hay que tener en cuenta que este clasificador de Naive Bayes solo funciona para datasets con dos valores de clase (positivo/negativo).\n",
    "\n",
    "\n",
    "- En el siguiente código, mostramos las tasas de confusión pedidas para el análisis ROC mediante una gráfica por cada caso de los clasificadores, donde el eje X es la tasa FPR y el eje Y es la tasa TPR. FPR indica la tasa de falsos positivos sobre la suma de FP y TN, y TPR la tasa de verdaderos positivos sobre la suma de TP y FN. También, dibujamos la bisectriz del espacio ROC para comprobar que nuestro clasificador funciona correctamente, y para ello los puntos deben quedar por encima de la bisectriz. Si no fuera así, el clasificador está haciendo más clasificaciones incorrectas que correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dedyVc/7H8dfHXdGCqDBlGX6SSptuW5RoREj2JYQxYpKxJg3GGGMbyxiUhMY6mDFF9tAgS7hT2ohbUqkoS7Sg+74/vz++J/dxu5fTfd/Xuc7yfj4e5+Fc17nOOZ9zuTuf8/1+r+/na+6OiIhIVTaIOwAREclsShQiIlItJQoREamWEoWIiFRLiUJERKqlRCEiItVSopCcZmYnmtnEuOMQyWZKFFLvzGy+ma0xs5VJt9vjiMXdH3L3vrV5boXPsdTM7jWzZhWO6WFmk8zsOzNbYWZPmlmHCsdsYma3mNmCxGsVJ7ZbVvG+ZmZDzWyGma1OvPfLZnZ8bT6HSF0pUUhU+rt7s6Tb0LgDqqX+7t4M6Ap0A0ase8DM9gImAk8ArYHtgfeA181sh8QxjYCXgI7AQcAmQA/gS2D3Kt7zVuA84EKgBdAGuCzx/PVmZg1q8zyRn7i7brrV6w2YD/ymisd2BF4BVgDLgUeTHnPgD8C8xGM3ABskHvs/YBLhC3Y58BDQPOm52wDjgGWJY25P7D8VeC3puB7AO4n3fwfokernAP4GPJ20PRkYVcnzngXuT9z/HfA50CzFc7cTUAoU1nDcpsA9wBLgM+CvQEHSZ34d+DvwVeKxe4FRidhWJh7fCrgF+Br4AOiW9PqXAB8D3wFzgCOSHjsVeA24MfHcT4B+cf/d6RbdTS0KSberCL/CNwO2Bm6r8PgRQCGwKzAA+G1ivwHXEn65tyckhj8DmFkB8BTwKfBrwi/wRyq+sZltDjxN+MXeArgZeNrMWtQUtJltDfQDihPbTQhJ5z+VHP5v4IDE/d8Az7n7ypreI2F/YKG7F9Vw3H1ACSHxdgP6EpLSOnsQEu4WwNWJfccSWiYtgR+AN4F3E9uPEc7HOh8DPQkJ6UrgQTP7VYXXn5t47t+Ae8zMUvyMkmWUKCQqj5vZN0m3MxL71wLbAa3d/Xt3f63C865396/cfQHh1+4JAO5e7O4vuPsP7r6M8KW2b+I5uxMSyDB3X1XF6wIcAnzk7g+4e4m7P0z4Jd2/hs/xHbAQ+AK4IrF/c8K/nyWVPGcJ4QsUQkKq7JiqtASWJu8ws0WJc/i9mW1nZlsSktZ5ic/7BaH1kDyGsdjdb0t8zjWJfePdfaq7fw+MB7539/vdvRR4lJBwAHD3/7j7Yncvc/dHgY/4eVfZp+5+V+K59wG/ArZcj88pWUSJQqJyuLs3T7rdldh/MaF18LaZzTaz31Z43sKk+58SEgBmtoWZPWJmn5nZt8CDlH8Zb0P44iqpIabWiddM9imhBVLd59gY6A3snPSeXwNlhC/Iin5F6B6D0A1W2TFV+cXx7r514n03JJy77YCGwJJ1iRi4k9B6WCf5PK7zedL9NZVs/zRQb2aDzGx60uvvQvlnh6Rk5u6rE3d/NtAvuUOJQtLK3Ze6+xnu3ho4ExhlZjsmHbJN0v1tgcWJ+9cSxjA6u/smwEmEL00IX4rbpjBou5jwJZtsW0Iff01xv0Lo578xsb2K0HVzTCWHH0sYwAZ4ETjQzJrW9B4Jk4CtzaywmmMWErqOWiYl4k3cvWNyyCm+3y+Y2XbAXcBQoIW7NwdmUX6+Jc8oUUhamdkxif5+CL/KnTB4u84wM9vMzLYBziV0iQBsTBiE/cbM2gDDkp7zNqF75zoza2pmG5nZ3pW8/TPATmY20MwamNlxQAfC+EYqbgEOMLOuie1LgFPM7A9mtnEi7r8CexH69QEeIHyx/9fMdjazDcyshZn90cwOrvgG7j6X0Dp4xMwOMLPGiTGYHknHLCGM89yUuPR2AzP7PzPbt+Lr1VJTwv+XZQBmdhqhRSF5SolCovJkhXkU4xP7dwPeMrOVwATgXHf/JOl5TwBTgemEged7EvuvJAxwr0jsH7fuCYl+8v6Egd0FwCLguIoBufuXwKGEy06/JHSDHeruyyseW5nE2Mj9wOWJ7deAA4EjCYnqU0I//z7u/lHimB8IA9ofAC8A3xISW0vgrSre6mzCgPvNhKuWFhEuAjgu8fkABgGNCFckfU0YjF6fLq7qPucc4CZCi+lzoBPhKinJU+auhYskM5iZA23dvTjuWESknFoUIiJSrcgShZmNNbMvzGxWFY+bmd2aKGcww8x2jSoWERGpvShbFPdSfcmBfkDbxG0wcEeEsUgWcHdTt5NI5oksUbj7q4SBuKoMIJQ5cHefAjSvMPNTREQyQJzFwtrw80lBixL7fjGL1cwGE1odNG3atPvOO++clgBFRLLekiWwdClTy8qWu3ur2rxEnImissk7lV6C5e5jgDEAhYWFXlRUUxkcEZE85w5mMGECTJyIjRxZsSpByuK86mkRP5+FuzXls3BFRKQ2vv4aTj8drrkmbB92GNxet+Vg4kwUE4BBiauf9gRWJGaciohIbYwfDx06wH33wdq19faykXU9mdnDhEJqLc1sEaHqZkMAdx9NKKdwMKFs82rgtKhiERHJaZ9/DuecA//5D3TtCk8/DbvW34yDyBKFu59Qw+NOKFUgIiJ1sXBhSA5XXw3DhkHDhvX68loiUUQkG336KTz5JAwdCoWFsGABtKhxDa5aUQkPEZFsUlYGI0fCLrvAiBHh8leILEmAEoWISPaYOxf23Te0IvbeG2bNgl9FP09ZXU8iItlg9WrYZx8oLYV774VBg8I8iTRQohARyWQffght20KTJvDAA+Gqpq22SmsI6noSEclE338Pl14a5kU89FDYd9BBaU8SoBaFiEjmef31MLt67lw47TQ45JBYw1GLQkQkk1x1FfTsGVoUzz8PY8fCZpvFGpIShYhIJli3LHXXrmGW9axZ0LdvvDElKFGIiMTpq6/glFPgr38N2/37wz/+Ac2axRtXEiUKEZG4PPYYtG8P//pXeYsiA2kwW0Qk3ZYsCZPmxo2D7t1h4kTo0iXuqKqkFoWISLotXhwGqq+/HqZMyegkAWpRiIikx/z5oYjfOeeEVsTChbFfzZQqtShERKJUWgq33hqK+F16KSxdGvZnSZIAJQoRkei8/z706gXnnhvmRsyaFcvM6rpS15OISBRWrw5JoqwM7r8fTjopbUX86psShYhIffrgA2jXLhTxe+ihMFC95ZZxR1Un6noSEakPa9bA8OHQsWN5Eb++fbM+SYBaFCIidffqq/C738FHH4X/Hnpo3BHVK7UoRETq4sorw6pzJSXw4otw113QvHncUdUrJQoRkdpYV3KjsBDOPx9mzoQ+feKNKSJKFCIi62P5cjj55FAOHMJaETffDE2bxhtXhJQoRERS4Q7//ndYce6RR2CD/Pn61GC2iEhNFi+GIUPgiSdCV9OLL0LnznFHlTb5kxJFRGpr6VKYNAluuAHefDOvkgSoRSEiUrl582DCBDjvPNh1V1iwIOeuZkqVWhQiIslKS+Hvfw9F/K64oryIX54mCVCiEBEpN3s27L03XHAB7L9/2M7CIn71TV1PIiIQivjtu28o3Pevf8Hxx2dtEb/6pkQhIvltzpywbnWTJuGy1y5doFWruKPKKOp6EpH8tHo1DBsGnTrBgw+Gfb/5jZJEJdSiEJH88/LLcMYZUFwMZ54Jhx0Wd0QZTS0KEckvV1wB++0XZlpPmgSjR8Omm8YdVUZTohCR/LCuiN/uu8OFF8KMGSFhSI0iTRRmdpCZzTWzYjO7pJLHNzWzJ83sPTObbWanRRmPiOShZctg4ED4y1/C9iGHwI03hsFrSUlkicLMCoCRQD+gA3CCmXWocNjZwBx37wL0Bm4ys0ZRxSQiecQ9XObavj089hg00ldLbUXZotgdKHb3ee7+I/AIMKDCMQ5sbGYGNAO+AkoijElE8sGiRWGA+sQTYccdYdo0GDEi7qiyVpSJog2wMGl7UWJfstuB9sBiYCZwrruXVXwhMxtsZkVmVrRs2bKo4hWRXLFsWVie9Oab4fXXwzrWUmtRJorKpjR6he0DgelAa6ArcLuZbfKLJ7mPcfdCdy9spWucRaQyxcWhRhNAt26wcGFYea6gIN64ckCUiWIRsE3S9taElkOy04BxHhQDnwA7RxiTiOSakpIwON2pU1i/+vPPw/5NfvGbU2opykTxDtDWzLZPDFAfD0yocMwCoA+AmW0JtAPmRRiTiOSSmTOhR48ww7pv31DEb8st444q50Q2M9vdS8xsKPA8UACMdffZZnZW4vHRwFXAvWY2k9BVNdzdl0cVk4jkkNWrwzyIDTYINZqOPVZF/CISaQkPd38GeKbCvtFJ9xcDfaOMQURyzKxZYXC6SRN49NFQxK9ly7ijymmamS0i2WHVqrBOROfO5UX8+vRRkkgDFQUUkcz30kuhiN8nn8CQITCg4pQsiZJaFCKS2S6/PJT/btAAXnkFRo7UFU1ppkQhIpmpLDH3tkcPuPhieO896NUr3pjylBKFiGSWL74Iy5BeeWXY7tcPrr8eGjeON648pkQhIpnBPQxSt28P48erumsGUaIQkfgtXAiHHgonnwzt2oUifsOHxx2VJChRiEj8vvwyFO/7xz9g8mToUHFFAomTLo8VkXh8+CFMmAAXXQRdu4ZWxcYbxx2VVEItChFJr5KSMDjduTNcfXV5ET8liYylRCEi6fPee7DHHnDJJXDwwTBnjor4ZQF1PYlIeqxeHUpuNGgQliY96qi4I5IUKVGISLRmzAhrRTRpAv/5Tyjit/nmcUcl60FdTyISjZUr4dxzw0D1Aw+EffvtpySRhdSiEJH698ILMHgwzJ8PQ4fCEUfEHZHUgVoUIlK/Lr00rDa34YZhTsRtt+mKpiynRCEi9WNdEb999oERI2D69HBfsp4ShYjUzdKlcPTR8Oc/h+1+/eCaa2CjjWINS+qPEoWI1I473HtvKLfx1FNaIyKHaTBbRNbfp5+GweqJE0P30t13h2J+kpPUohCR9ffNN/DOO3D77WHVOSWJnKYWhYikZu7cUMRv2LAwaW7BAmjWLO6oJA3UohCR6q1dC9deG5LDddeFFehASSKPKFGISNWmTQtF/P74R+jfPxTx22KLuKOSNFPXk4hUbvVqOOAAaNgQ/vtfOPLIuCOSmChRiMjPTZsW6jM1aRKqvHbpApttFndUEiN1PYlI8N13oS7TrruWF/Hr3VtJQtSiEBHguefgzDPDcqTnnqtuJvkZtShE8t2IEaHsRtOm8PrrcMstuqJJfkYtCpF8VVoKBQWhe6lBA7jsslDxVaQCtShE8s2SJaFraV0RvwMPhKuuUpKQKilRiOQLd/jnP0MRv2ef1SC1pExdTyL5YP58OOMMePFF6NkzFPHbaae4o5IsoRaFSD5YsQLefRdGjYKXX1aSkPUSaaIws4PMbK6ZFZvZJVUc09vMppvZbDN7Jcp4RPLKnDmhNhOUF/H7/e9hA/0+lPUT2V+MmRUAI4F+QAfgBDPrUOGY5sAo4DB37wgcE1U8Innjxx/hr3+Fbt3gxhvLi/g1bRpvXJK1ovxpsTtQ7O7z3P1H4BFgQIVjBgLj3H0BgLt/EWE8IrmvqAh22w0uvzxc2aQiflIPokwUbYCFSduLEvuS7QRsZmYvm9lUMxtU2QuZ2WAzKzKzomXLlkUUrkiWW7UqXOq6fDk88QQ8/LCShNSLKK96skr2eSXv3x3oAzQG3jSzKe7+4c+e5D4GGANQWFhY8TVE8tu774Yifk2bwvjx0LkzNG8ed1SSQ6JsUSwCtkna3hpYXMkxz7n7KndfDrwKdIkwJpHc8e23MGQIdO8ODz4Y9vXqpSQh9S7KRPEO0NbMtjezRsDxwIQKxzwB9DSzBmbWBNgDeD/CmERywzPPQMeOcOedcMEFcNRRcUckOSyyrid3LzGzocDzQAEw1t1nm9lZicdHu/v7ZvYcMAMoA+5291lRxSSSE4YPh7/9LcywfuyxsAKdSIQinZnt7s8Az1TYN7rC9g3ADVHGIZL13KGsLBTx69MHNtooLE+q+kySBpp5I5LpPvsMDj8crrgibPftC1deqSQhaaNEIZKp3OGuu0IX08SJ0LJl3BFJnlJRQJFM9MkncPrp8L//hfUi7roLdtwx7qgkTylRiGSilSthxoxwVdPvfqf6TBIrJQqRTDFrFkyYEAapO3UKRfyaNIk7KhGNUYjE7scfw+D0rrvC3/9eXsRPSUIyhFoUGeDxaZ9xw/NzWfzNGlo3b8ywA9txeLeKZbEkJ73zDvz2t6E1MXAg3HILtGoVd1QiP6NEEbPHp33GiHEzWbO2FIDPvlnDiHEzAZQsct2qVXDQQdC4cehy6t8/7ohEKrXeXU9mVmBmJ0YRTD664fm5PyWJddasLeWG5+fGFJFErqgoTJ5r2jRUeZ09W0lCMlqVicLMNjGzEWZ2u5n1teAcYB5wbPpCzG2Lv1mzXvsli61YAWeeGdaLWFfEb599YNNN441LpAbVdT09AHwNvAn8DhgGNAIGuPv0NMSWF1o3b8xnlSSF1s0bxxCNRObJJ+Gss2DpUrjoIjj66LgjEklZdV1PO7j7qe5+J3ACUAgcqiRRv4Yd2I7GDQt+tq9xwwKGHdgupoik3g0bBocdBi1awJQpcMMNuqJJskp1LYq16+64e6mZfeLu36UhpryybsBaVz3lGHcoLYUGDUJtpk02CVVfGzWKOzKR9WbulS8YZ2alwCrKV6prDKxObLu7b5KWCCsoLCz0oqKiON5aJDWLFsHvfx9Wmrv66rijEQHAzKa6e2Ftnltl15O7F7j7Ju6+ceLWIGk7liQhktHKykLJjQ4dYNIk2GqruCMSqRdVdj2Z2UbAWcCOhIWFxrp7SboCE8kq8+aFiXOvvBLWixgzBnbYIe6oROpFdWMU9xHGKSYDBwMdgXPTEVS200zrPLRqFcyZA3ffHRKGWc3PEckS1SWKDu7eCcDM7gHeTk9I2U0zrfPIzJlhwtxll4Uifp9+GmZZi+SY6i6PTb7qSV1OKdJM6zzwww/wpz+FIn633lpexE9JQnJUdS2Krmb2beK+AY0T27Fe9ZTpNNM6x02ZEhYUmjMHTj45VHtt0SLuqEQiVV2ieM/du6UtkhyhmdY5bNUqOOSQUKPpmWegX7+4IxJJi+q6niqfYCHV0kzrHPTWW+VF/J58MhTxU5KQPFJdi2ILM7ugqgfd/eYI4sl6mmmdQ775JtRluuceuO8+GDQIevSIOyqRtKsuURQAzSifmS0pOrxbGyWGbPf44zBkSBioHj4cjjkm7ohEYlNdolji7n9JWyQimeKCC8IgdZcuoaupe/e4IxKJVXWJQi0JyR/JRfwOPjhcyXTxxdCwYdyRicSuukTRJ21RyM9oZneaLVgQ1oro1i0U8fvNb8JNRIDqiwJ+lc5AJFg3s/uzb9bglM/sfnzaZ3GHlnvKymDUKOjYMdRoat067ohEMtJ6r5kt0dLM7jQpLobeveHss2GvvcIlr2efHXdUIhmpuq4niYFmdqfJ99/Dhx/CP/8Jp5yiIn4i1VCLIsNUNYNbM7vrwfTpcOWV4f4uu8D8+XDqqUoSIjVQoqijx6d9xt7XTWL7S55m7+sm1XksQTO7I/D993DppVBYCHfcUV7Eb6ON4o1LJEuo66kOoigprpnd9eyNN0IRvw8+CF1MN98Mm28ed1QiWUWJog6qG3iuyxe7ZnbXk1WroH9/aNYMnnsODjww7ohEspISRR1o4DlDvfkm7LFHKOL31FNhPGLjjeOOSiRrRTpGYWYHmdlcMys2s0uqOW43Mys1s6OjjKe+aeA5w3z9dViGtEcPeOCBsG+vvZQkROooskRhZgXASKAf0AE4wcw6VHHc9cDzUcUSFQ08Z5Bx46BDB7j/fhgxAo47Lu6IRHJGlC2K3YFid5/n7j8CjwADKjnuHOC/wBcRxhKJw7u14dojO9GmeWMMaNO8Mdce2UnjC+l2/vlw1FGw1VbwzjtwzTW6okmkHkU5RtEGWJi0vQjYI/kAM2sDHAHsD+xW1QuZ2WBgMMC2225b74HWhQaeY5JcxO/QQ2GLLcLaESriJ1LvomxRVDaLqeKqebcAw929tJJjy5/kPsbdC929sFWrVvUWoGSp+fPhoIPg8svDdp8+obtJSUIkElEmikXANknbWwOLKxxTCDxiZvOBo4FRZnZ4hDFJNisrg9tuC1cxvfEGbLdd3BGJ5IUou57eAdqa2fbAZ8DxwMDkA9x9+3X3zexe4Cl3fzzCmCRbffQRnHYavP56aE2MHq1EIZImkSUKdy8xs6GEq5kKgLHuPtvMzko8Pjqq95Yc9OOP8PHH4aqmk05SfSaRNDL3isMGma2wsNCLioriDkPSYdo0eOIJ+POfw/YPP8CGG8Yakki2MrOp7l5Ym+eqKKBknu+/D4PTu+0Gd94Jy5aF/UoSIrFQopDM8tpr0KULXHcdDBoEc+aArnQTiZVqPUnmWLkSBgyATTaBiRPhgAPijkhEUKKQTPDaa6E+U7Nm8PTT4fLXZs3ijkpEEtT1JPH58svQvdSzZ3kRvz33VJIQyTBqUUj6ucNjj8HQofDVV2GG9fHHxx2ViFRBiULS7/zz4R//gO7dw1hEly5xRyQi1VCikPRwh5KSUI/psMOgdWu44IJQ1E9EMprGKCR6n3wCffuWF/Hbf3+4+GIlCZEsoUQh0SktDV1Mu+wCb70FO+wQd0QiUgv6SSfR+PBDOPXUsH51v35hhvU229T4NBHJPEoUEo2SEvj0U3jwQRg4UEX8RLKYEoXUn6KiUMTvqqvC+tXz5qk+k0gO0BiF1N2aNWFweo89YOxYFfETyTFKFFI3r7wCnTvDDTfA6afD7Nkq4ieSY9T1JLW3ciUceSQ0bw4vvRQuexWRnKNEIetv8mTYe+9Qk+nZZ6FjR2jaNO6oRCQi6nqS1C1fHpYh7dWrvIjf7rsrSYjkOLUopGbu8O9/wznnwNdfwxVXqIifSB5RopCanXsu3HZbWJr0pZegU6e4IxKRNFKikMq5w9q10KgRHHEEbLcdnHceFBTEHZmIpJnGKOSXPv4Y+vSByy4L2/vtBxdeqCQhkqeUKKRcaSncfHPoWpo6Fdq1izsiEckA6nqS4IMP4JRT4O23oX9/uOMOaNMm7qhEJAMoUUhQVgaLF8PDD8Nxx6mIn4j8RIkin739dijid/XVoYjfxx+HwWsRkSQao8hHq1fDRRfBXnvBffeVF/FTkhCRSihR5Jv//S8MVt90E5xxhor4iUiN1PWUT1auhGOOCUX8/vc/6N077ohEJAuoRZEPXn45DFavK+I3Y4aShIikTIkily1bBiecECbMPfhg2LfbbtCkSbxxiUhWUddTLnIPl7n+4Q/w3XdhaVIV8RORWlKiyEXnnAMjR8Kee8I994RLX0VEakmJIleUlUFJSbjE9eijYccdQ8JQfSYRqaNIxyjM7CAzm2tmxWZ2SSWPn2hmMxK3N8ysS5Tx5KyPPgrLkF56adju3VuVXkWk3kSWKMysABgJ9AM6ACeYWcU+kE+Afd29M3AVMCaqeHJSSQnceCN07gzTp0P79nFHJCI5KMqup92BYnefB2BmjwADgDnrDnD3N5KOnwJsHWE8ueX992HQICgqggEDYNQoaN067qhEJAdF2fXUBliYtL0osa8qpwPPVvaAmQ02syIzK1q2rtyEwOefw6OPwvjxShIiEpkoE0Vl5Ue90gPN9iMkiuGVPe7uY9y90N0LW+VzuYkpU2DEiHC/fftQxO/YY1XpVUQiFWWiWARsk7S9NbC44kFm1hm4Gxjg7l9GGE/2WrUKzj8fevSAhx4qL+LXsGG8cYlIXogyUbwDtDWz7c2sEXA8MCH5ADPbFhgHnOzuH0YYS/Z68UXYZRe45RYYMkRF/EQk7SIbzHb3EjMbCjwPFABj3X22mZ2VeHw08CegBTDKQvdJibsXRhVT1lm5Msyo3nxzePVV6Nkz7ohEJA+Ze6XDBhmrsLDQi4qK4g4jWpMmwb77hnkQU6eGmdWNG8cdlYhkMTObWtsf4ioKmEk+/zwMTvfpU17Er3t3JQkRiZUSRSZwhwceCC2HdUuTDhwYd1QiIoBqPWWGs8+GO+4IS5Pec49mWItIRlGiiEtZGaxdCxtuCMcdF5LDkCGqzyQiGUddT3GYOzcMVq8r4rfvvqr0KiIZS4kindauheuugy5dYNYs6NQp7ohERGqkrqd0mT0bTj4Zpk2DI48MCwtttVXcUYmI1EiJIl0KCuCrr+Cxx+Coo+KORkQkZep6itIbb8DwRJ3DnXeG4mIlCRHJOkoUUVi5Ev7wB9hnn1AGfPnysL+BGnAikn2UKOrbxImhiN/tt8PQoWHQumXLuKMSEak1/cStTytXwoknQosWMHky7L133BGJiNSZWhT14YUXoLQUmjULLYrp05UkRCRnKFHUxZIlYXC6b9+woBBAt26w0UbxxiUiUo+UKGrDHe69NxTxe/rpMIlORfxEJEdpjKI2fv97uPPOcFXT3XdDu3ZxRyQiEhklilQlF/EbOBA6d4azzoIN1CgTkdymb7lUvP9+WIb0j38M2716hUqvShIikgf0TVedtWvhmmuga1f44IMwUC0ikmfU9VSV2bPhpJPCpa7HHAO33QZbbhl3VCIiaadEUZUGDWDFChg3Do44Iu5oRERio66nZJMnw0UXhfvt2sGHHypJiEjeU6IA+O67sG51r16hBaEifiIiP1GiePZZ6NgR7rgDzjsPZs5UET8RkST5/ZP5u+9g0CDYYouwdsSee8YdkYhIxsm/FoU7PPdcKOK38cbw4ovw7rtKEiIiVcivRLFkSVivul+/8iJ+XbqE2dYiIlKp/EgU7jB2LLRvH1oTf/ubiviJiKQoP8YozjoLxowJVzXdfTe0bRt3RCIiWSN3E0VpaSjBsdFGYYZ1t24weLDqM4mIrKfc/NacPTusMLeuiF/Pnqr0KiJSS7n1zfnjj3DVVaH1UFwMu+0Wd0QiIlkvd7qeZs6EE08M/z3+eLj1VmjVKu6oRESyXu4kikaNYFtEzVoAAAV7SURBVPVqeOIJOOywuKMREckZ2d319MorcOGF4X67djB3rpKEiEg9izRRmNlBZjbXzIrN7JJKHjczuzXx+Awz2zWlF/7227Bude/e8Pjj5UX8CgrqM3wRESHCRGFmBcBIoB/QATjBzDpUOKwf0DZxGwzcUeMLr1gRiviNGQMXXKAifiIiEYuyRbE7UOzu89z9R+ARYECFYwYA93swBWhuZr+q9lXnz4dNNw1F/G66CZo0iSJ2ERFJiHIwuw2wMGl7EbBHCse0AZYkH2RmgwktDoAfbPbsWSriB0BLYHncQWQInYtyOhfldC7KtavtE6NMFFbJPq/FMbj7GGAMgJkVuXth3cPLfjoX5XQuyulclNO5KGdmRbV9bpRdT4uAbZK2twYW1+IYERGJUZSJ4h2grZltb2aNgOOBCRWOmQAMSlz9tCewwt2XVHwhERGJT2RdT+5eYmZDgeeBAmCsu882s7MSj48GngEOBoqB1cBpKbz0mIhCzkY6F+V0LsrpXJTTuShX63Nh7r8YEhAREflJds/MFhGRyClRiIhItTI2UURW/iMLpXAuTkycgxlm9oaZdYkjznSo6VwkHbebmZWa2dHpjC+dUjkXZtbbzKab2WwzeyXdMaZLCv9GNjWzJ83svcS5SGU8NOuY2Vgz+8LMZlXxeO2+N909426Ewe+PgR2ARsB7QIcKxxwMPEuYi7En8Fbcccd4LnoAmyXu98vnc5F03CTCxRJHxx13jH8XzYE5wLaJ7S3ijjvGc/FH4PrE/VbAV0CjuGOP4Fz0AnYFZlXxeK2+NzO1RRFN+Y/sVOO5cPc33P3rxOYUwnyUXJTK3wXAOcB/gS/SGVyapXIuBgLj3H0BgLvn6vlI5Vw4sLGZGdCMkChK0htm9Nz9VcJnq0qtvjczNVFUVdpjfY/JBev7OU8n/GLIRTWeCzNrAxwBjE5jXHFI5e9iJ2AzM3vZzKaa2aC0RZdeqZyL24H2hAm9M4Fz3b0sPeFllFp9b2bqwkX1Vv4jB6T8Oc1sP0Ki2CfSiOKTyrm4BRju7qXhx2POSuVcNAC6A32AxsCbZjbF3T+MOrg0S+VcHAhMB/YH/g94wcwmu/u3UQeXYWr1vZmpiULlP8ql9DnNrDNwN9DP3b9MU2zplsq5KAQeSSSJlsDBZlbi7o+nJ8S0SfXfyHJ3XwWsMrNXgS5AriWKVM7FacB1Hjrqi83sE2Bn4O30hJgxavW9maldTyr/Ua7Gc2Fm2wLjgJNz8NdishrPhbtv7+6/dvdfA48BQ3IwSUBq/0aeAHqaWQMza0Ko3vx+muNMh1TOxQJCywoz25JQSXVeWqPMDLX63szIFoVHV/4j66R4Lv4EtABGJX5Jl3gOVsxM8VzkhVTOhbu/b2bPATOAMuBud6/0sslsluLfxVXAvWY2k9D9Mtzdc678uJk9DPQGWprZIuAKoCHU7XtTJTxERKRamdr1JCIiGUKJQkREqqVEISIi1VKiEBGRailRiIhItZQoRFKUqEY7Pen260R11hVmNs3M3jezKxLHJu//wMxujDt+kdrKyHkUIhlqjbt3Td5hZr8GJrv7oWbWFJhuZk8lHl63vzEwzczGu/vr6Q1ZpO7UohCpJ4lSGVMJtYSS968h1BnKxaKVkgeUKERS1zip22l8xQfNrAWhxv/sCvs3A9oCr6YnTJH6pa4nkdT9ouspoaeZTSOUybguUT6id2L/DEJdoevcfWkaYxWpN0oUInU32d0PrWq/me0EvJYYo5ie7uBE6kpdTyIRS1T0vRYYHncsIrWhRCGSHqOBXma2fdyBiKwvVY8VEZFqqUUhIiLVUqIQEZFqKVGIiEi1lChERKRaShQiIlItJQoREamWEoWIiFTr/wEbaXSylUcF9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(dicc_tasas)\n",
    "# [TPR, FNR, FPR, TNR]\n",
    "\n",
    "# Espacio ROC usando los clasificadores NB sobre el dataset de Tic-Tac-Toe.\n",
    "x = []\n",
    "x.append(dicc_tasas['Laplace']['VS']['TTT'][2])\n",
    "x.append(dicc_tasas['Laplace']['VC']['TTT'][2])\n",
    "x.append(dicc_tasas['NoLaplace']['VS']['TTT'][2])\n",
    "x.append(dicc_tasas['NoLaplace']['VC']['TTT'][2])\n",
    "\n",
    "y = []\n",
    "y.append(dicc_tasas['Laplace']['VS']['TTT'][0])\n",
    "y.append(dicc_tasas['Laplace']['VC']['TTT'][0])\n",
    "y.append(dicc_tasas['NoLaplace']['VS']['TTT'][0])\n",
    "y.append(dicc_tasas['NoLaplace']['VC']['TTT'][0])\n",
    "\n",
    "title('Espacio ROC Tic-Tac-Toe') \n",
    "xlabel('FPR')      \n",
    "ylabel('TPR')    \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plot(x, y, 'o')                  \n",
    "show()\n",
    "\n",
    "# Espacio ROC usando los clasificadores NB sobre el dataset de German.\n",
    "x = []\n",
    "x.append(dicc_tasas['Laplace']['VC']['GER'][2])\n",
    "x.append(dicc_tasas['Laplace']['VS']['GER'][2])\n",
    "x.append(dicc_tasas['NoLaplace']['VC']['GER'][2])\n",
    "x.append(dicc_tasas['NoLaplace']['VS']['GER'][2])\n",
    "\n",
    "y = []\n",
    "y.append(dicc_tasas['Laplace']['VC']['GER'][0])\n",
    "y.append(dicc_tasas['Laplace']['VS']['GER'][0])\n",
    "y.append(dicc_tasas['NoLaplace']['VC']['GER'][0])\n",
    "y.append(dicc_tasas['NoLaplace']['VS']['GER'][0])\n",
    "\n",
    "title('Espacio ROC German') \n",
    "xlabel('FPR')      \n",
    "ylabel('TPR')  \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plot(x, y, 'o')                  \n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos observar que nuestro clasificador NB funciona correctamente usando ambos datasets, ya que realiza buenas clasificaciones. Esto se refleja en los espacios ROC donde los puntos de las tasas quedan por encima de la bisectriz, como hemos comentado antes. Es decir, la mayor parte de las clasificaciones del clasificador son correctas al haber más verdaderos que falsos.\n",
    "\n",
    "\n",
    "- Observando los espacios hemos notado que la posición de los puntos indica la cantidad de clasificaciones positivas o negativas del clasificador. Si se encuentran más pegadas al 1 del eje TPR quiere decir que se han clasificado más datos como positivos (como se ve en Tic-Tac-Toe). Y, por el contrario, si se encuentran más cercanos al 0 de TPR (por debajo del 0.5 de TPR), quiere decir que se han clasificado más datos como negativos (como se ve en German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
